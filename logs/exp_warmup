Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='adam', schedule_lr=True, freeze_layers=0, inter_rep=0, combine_reps=False, warmup=True)
Total number of model parameters is 94402472
TRAIN LOADER LENGTH/NUMBER OF STEPS,  924
warmup_milestone 924.0
total_steps 9240, first_milestone 3696.0, second_milestone 5544.0
EPOCH 1:
isNotFrozen  True
  batch 50 loss: 11.124834728240966
  batch 100 loss: 5.604850730895996
  batch 150 loss: 3.5586064577102663
  batch 200 loss: 3.3915388631820678
  batch 250 loss: 3.3439326763153074
  batch 300 loss: 3.307452483177185
  batch 350 loss: 3.2898036813735962
  batch 400 loss: 3.2927225971221925
  batch 450 loss: 3.26244131565094
  batch 500 loss: 3.2206174421310423
  batch 550 loss: 3.147286643981934
  batch 600 loss: 2.993493089675903
  batch 650 loss: 2.585247688293457
  batch 700 loss: 2.2018249678611754
  batch 750 loss: 1.761058657169342
  batch 800 loss: 1.3090949368476867
  batch 850 loss: 1.100265499353409
  batch 900 loss: 0.9307695901393891
first 5 warmup etas, [], last 5 warmup etas, []
last epoch eta,  [9.999999999999977e-05]
LOSS train 0.93077 valid 0.63279, valid PER 14.99%
EPOCH 2:
isNotFrozen  True
  batch 50 loss: 0.7676216512918472
  batch 100 loss: 0.7289494556188584
  batch 150 loss: 0.6671510702371597
  batch 200 loss: 0.6543651992082595
  batch 250 loss: 0.6229663503170013
  batch 300 loss: 0.5668204015493393
  batch 350 loss: 0.6083953142166137
  batch 400 loss: 0.5752876353263855
  batch 450 loss: 0.5250087827444077
  batch 500 loss: 0.5531455981731415
  batch 550 loss: 0.529374035000801
  batch 600 loss: 0.5090240919589997
  batch 650 loss: 0.5148459368944168
  batch 700 loss: 0.5073618084192276
  batch 750 loss: 0.5183140707015991
  batch 800 loss: 0.4718546998500824
  batch 850 loss: 0.47969203561544416
  batch 900 loss: 0.4689645254611969
last epoch eta,  [9.999999999999977e-05]
LOSS train 0.46896 valid 0.33850, valid PER 9.86%
EPOCH 3:
isNotFrozen  True
  batch 50 loss: 0.40914732962846756
  batch 100 loss: 0.4073956114053726
  batch 150 loss: 0.42953980088233945
  batch 200 loss: 0.4090553015470505
  batch 250 loss: 0.4211495763063431
  batch 300 loss: 0.40344215095043184
  batch 350 loss: 0.42825297117233274
  batch 400 loss: 0.40682932645082476
  batch 450 loss: 0.48220467805862427
  batch 500 loss: 0.4442335048317909
  batch 550 loss: 0.44093640834093095
  batch 600 loss: 0.4368585303425789
  batch 650 loss: 0.40903246611356736
  batch 700 loss: 0.4393626636266708
  batch 750 loss: 0.4680862793326378
  batch 800 loss: 0.4562722599506378
  batch 850 loss: 0.43231723219156265
  batch 900 loss: 0.3995745274424553
last epoch eta,  [9.999999999999977e-05]
LOSS train 0.39957 valid 0.33084, valid PER 9.42%
EPOCH 4:
isNotFrozen  True
  batch 50 loss: 0.38743115693330765
  batch 100 loss: 0.3833211550116539
  batch 150 loss: 0.3987546050548553
  batch 200 loss: 0.410675595998764
  batch 250 loss: 0.44036353945732115
  batch 300 loss: 0.40867999017238615
  batch 350 loss: 0.40723309308290484
  batch 400 loss: 0.4776584017276764
  batch 450 loss: 0.40422255963087084
  batch 500 loss: 0.3969599461555481
  batch 550 loss: 0.39870062440633774
  batch 600 loss: 0.4529621666669846
  batch 650 loss: 0.3953500086069107
  batch 700 loss: 0.39719696313142777
  batch 750 loss: 0.44105192869901655
  batch 800 loss: 0.42420719176530836
  batch 850 loss: 0.4105008834600449
  batch 900 loss: 0.4336600613594055
last epoch eta,  [9.999999999999977e-05]
LOSS train 0.43366 valid 0.36556, valid PER 10.90%
EPOCH 5:
isNotFrozen  True
  batch 50 loss: 0.3468871656060219
  batch 100 loss: 0.3821308010816574
  batch 150 loss: 0.39165581792593
  batch 200 loss: 0.36293151557445524
  batch 250 loss: 0.3606920936703682
  batch 300 loss: 0.375681409239769
  batch 350 loss: 0.36236126601696017
  batch 400 loss: 0.35455009698867795
  batch 450 loss: 0.34558647364377976
  batch 500 loss: 0.4621258533000946
  batch 550 loss: 0.3847980099916458
  batch 600 loss: 0.44757095009088516
  batch 650 loss: 0.6103324377536774
  batch 700 loss: 0.5736019587516785
  batch 750 loss: 0.45650984287261964
  batch 800 loss: 0.45359923779964445
  batch 850 loss: 0.4148929509520531
  batch 900 loss: 0.4085787969827652
last epoch eta,  [8.333333333333386e-05]
LOSS train 0.40858 valid 0.32307, valid PER 9.43%
EPOCH 6:
isNotFrozen  True
  batch 50 loss: 0.3644827100634575
  batch 100 loss: 0.30465734779834747
  batch 150 loss: 0.2994175328314304
  batch 200 loss: 0.34796035826206206
  batch 250 loss: 0.32919351398944857
  batch 300 loss: 0.3209513859450817
  batch 350 loss: 0.3103656044602394
  batch 400 loss: 0.3016501808166504
  batch 450 loss: 0.2801640336215496
  batch 500 loss: 0.2879621708393097
  batch 550 loss: 0.28195570975542067
  batch 600 loss: 0.2948807802796364
  batch 650 loss: 0.3327417758107185
  batch 700 loss: 0.2924791958928108
  batch 750 loss: 0.3054382368922234
  batch 800 loss: 0.3169423106312752
  batch 850 loss: 0.3041080318391323
  batch 900 loss: 0.2886973056197166
last epoch eta,  [6.666666666666794e-05]
LOSS train 0.28870 valid 0.34932, valid PER 10.14%
EPOCH 7:
isNotFrozen  True
  batch 50 loss: 0.3146191844344139
  batch 100 loss: 0.28915774941444394
  batch 150 loss: 0.24538918763399123
  batch 200 loss: 0.273661447763443
  batch 250 loss: 0.23551078498363495
  batch 300 loss: 0.2608326584100723
  batch 350 loss: 0.261001365929842
  batch 400 loss: 0.2617729341983795
  batch 450 loss: 0.27534852892160416
  batch 500 loss: 0.2781709086894989
  batch 550 loss: 0.25166216894984245
  batch 600 loss: 0.24357559621334077
  batch 650 loss: 0.2580407474935055
  batch 700 loss: 0.26230514869093896
  batch 750 loss: 0.22065053537487983
  batch 800 loss: 0.25320113882422446
  batch 850 loss: 0.26144305750727653
  batch 900 loss: 0.2533594807982445
last epoch eta,  [5.00000000000014e-05]
LOSS train 0.25336 valid 0.31131, valid PER 8.71%
EPOCH 8:
isNotFrozen  True
  batch 50 loss: 0.2063794158399105
  batch 100 loss: 0.2261430197954178
  batch 150 loss: 0.22894765958189964
  batch 200 loss: 0.20094925165176392
  batch 250 loss: 0.21278753757476807
  batch 300 loss: 0.20102010920643806
  batch 350 loss: 0.21613861471414567
  batch 400 loss: 0.24390642300248147
  batch 450 loss: 0.2186152780056
  batch 500 loss: 0.20386108860373497
  batch 550 loss: 0.19620050802826883
  batch 600 loss: 0.21113283902406693
  batch 650 loss: 0.2130828432738781
  batch 700 loss: 0.1966522191464901
  batch 750 loss: 0.19220876023173333
  batch 800 loss: 0.22189080864191055
  batch 850 loss: 0.1941893734037876
  batch 900 loss: 0.21528393015265465
last epoch eta,  [3.333333333333492e-05]
LOSS train 0.21528 valid 0.28646, valid PER 7.99%
EPOCH 9:
isNotFrozen  True
  batch 50 loss: 0.1822306279838085
  batch 100 loss: 0.20271481454372406
  batch 150 loss: 0.181032987087965
  batch 200 loss: 0.18799636110663415
  batch 250 loss: 0.1890096342563629
  batch 300 loss: 0.18289675280451775
  batch 350 loss: 0.1859511224925518
  batch 400 loss: 0.19437718003988266
  batch 450 loss: 0.17269304081797598
  batch 500 loss: 0.18492896661162375
  batch 550 loss: 0.1754075449705124
  batch 600 loss: 0.1790957273542881
  batch 650 loss: 0.17603950887918474
  batch 700 loss: 0.17075800262391566
  batch 750 loss: 0.1789420300722122
  batch 800 loss: 0.17741533666849135
  batch 850 loss: 0.18113776057958603
  batch 900 loss: 0.16207232005894184
last epoch eta,  [1.6666666666667525e-05]
LOSS train 0.16207 valid 0.30061, valid PER 8.06%
EPOCH 10:
isNotFrozen  True
  batch 50 loss: 0.143154491558671
  batch 100 loss: 0.1597487022727728
  batch 150 loss: 0.16377721205353737
  batch 200 loss: 0.15085963927209378
  batch 250 loss: 0.14190752521157265
  batch 300 loss: 0.1540076258033514
  batch 350 loss: 0.1544605477899313
  batch 400 loss: 0.15004553630948067
  batch 450 loss: 0.14579919762909413
  batch 500 loss: 0.1567510436475277
  batch 550 loss: 0.17288277089595794
  batch 600 loss: 0.14199725195765495
  batch 650 loss: 0.15767432533204556
  batch 700 loss: 0.15388847649097442
  batch 750 loss: 0.1492360018193722
  batch 800 loss: 0.15042357839643955
  batch 850 loss: 0.15335725754499435
  batch 900 loss: 0.13297947220504283
last epoch eta,  [0.0]
LOSS train 0.13298 valid 0.30971, valid PER 7.99%
Training finished in 14.0 minutes.
Model saved to checkpoints/20240206_194433/model_8
Loading model from checkpoints/20240206_194433/model_8
CLEAN
 SUB: 5.77%, DEL: 2.07%, INS: 2.52%, COR: 92.16%, PER: 10.36%

