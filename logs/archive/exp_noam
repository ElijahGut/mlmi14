Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='noam', schedule_lr=False, freeze_layers=0, inter_rep=0, combine_reps=False, warmup=False)
Total number of model parameters is 94402472
EPOCH 1:
isNotFrozen  True
  batch 50 loss: 10.465843267440796
  batch 100 loss: 4.6909830236434935
  batch 150 loss: 3.517776026725769
  batch 200 loss: 3.370300359725952
  batch 250 loss: 3.3140488767623903
  batch 300 loss: 3.259946150779724
  batch 350 loss: 3.2255347919464112
  batch 400 loss: 3.1927307176589967
  batch 450 loss: 3.050426678657532
  batch 500 loss: 2.7753356885910034
  batch 550 loss: 2.2942191624641417
  batch 600 loss: 1.748944799900055
  batch 650 loss: 1.3613553905487061
  batch 700 loss: 1.1724811685085297
  batch 750 loss: 1.028818851709366
  batch 800 loss: 0.7845620763301849
  batch 850 loss: 0.667698849439621
  batch 900 loss: 0.6143239915370942
last epoch eta,  9.972631169354988e-05
LOSS train 0.61432 valid 0.39991, valid PER 11.34%
EPOCH 2:
isNotFrozen  True
  batch 50 loss: 0.5685503858327866
  batch 100 loss: 0.5594188356399536
  batch 150 loss: 0.5041715162992477
  batch 200 loss: 0.5147027242183685
  batch 250 loss: 0.5421645468473435
  batch 300 loss: 0.5080116790533066
  batch 350 loss: 0.5113739839196205
  batch 400 loss: 0.4876615738868713
  batch 450 loss: 0.4698129639029503
  batch 500 loss: 0.4825905740261078
  batch 550 loss: 0.49884876668453215
  batch 600 loss: 0.4703038087487221
  batch 650 loss: 0.49839285790920257
  batch 700 loss: 0.48856901317834855
  batch 750 loss: 0.5016520926356316
  batch 800 loss: 0.4067287003993988
  batch 850 loss: 0.46769682079553604
  batch 900 loss: 0.4577327650785446
last epoch eta,  9.937625282444121e-05
LOSS train 0.45773 valid 0.32608, valid PER 9.65%
EPOCH 3:
isNotFrozen  True
  batch 50 loss: 0.4043303966522217
  batch 100 loss: 0.37983864575624465
  batch 150 loss: 0.37896173387765886
  batch 200 loss: 0.3793983030319214
  batch 250 loss: 0.39905461996793745
  batch 300 loss: 0.3759086269140244
  batch 350 loss: 0.4109401935338974
  batch 400 loss: 0.3854978922009468
  batch 450 loss: 0.40997281670570374
  batch 500 loss: 0.4140654760599136
  batch 550 loss: 0.37505544602870944
  batch 600 loss: 0.37749860286712644
  batch 650 loss: 0.36158207774162293
  batch 700 loss: 0.39040427446365356
  batch 750 loss: 0.3854438671469688
  batch 800 loss: 0.519525793492794
  batch 850 loss: 0.45462285190820695
  batch 900 loss: 0.3618501704931259
last epoch eta,  9.902985458911241e-05
LOSS train 0.36185 valid 0.29982, valid PER 9.19%
EPOCH 4:
isNotFrozen  True
  batch 50 loss: 0.3649364513158798
  batch 100 loss: 0.31494667768478396
  batch 150 loss: 0.3217716473340988
  batch 200 loss: 0.3414697566628456
  batch 250 loss: 0.32464256435632705
  batch 300 loss: 0.3278654545545578
  batch 350 loss: 0.32655781537294387
  batch 400 loss: 0.31985593378543853
  batch 450 loss: 0.3386134865880013
  batch 500 loss: 0.334291578233242
  batch 550 loss: 0.32314422965049744
  batch 600 loss: 0.35692800670862196
  batch 650 loss: 0.32091389000415804
  batch 700 loss: 0.3301001989841461
  batch 750 loss: 0.32581849545240404
  batch 800 loss: 0.35177835941314695
  batch 850 loss: 0.33325393170118334
  batch 900 loss: 0.3371083816885948
last epoch eta,  9.868705362920246e-05
LOSS train 0.33711 valid 0.34157, valid PER 11.96%
EPOCH 5:
isNotFrozen  True
  batch 50 loss: 0.29577780187129976
  batch 100 loss: 0.31440180003643037
  batch 150 loss: 0.32476354092359544
  batch 200 loss: 0.28327380150556564
  batch 250 loss: 0.32123311728239057
  batch 300 loss: 0.2970531779527664
  batch 350 loss: 0.29975576579570773
  batch 400 loss: 0.3064801362156868
  batch 450 loss: 0.28375291734933855
  batch 500 loss: 0.31712047159671786
  batch 550 loss: 0.313411985039711
  batch 600 loss: 0.3050468111038208
  batch 650 loss: 0.3129183426499367
  batch 700 loss: 0.3177137529850006
  batch 750 loss: 0.28854072630405425
  batch 800 loss: 0.28852752327919007
  batch 850 loss: 0.30596817418932915
  batch 900 loss: 0.31562605381011966
last epoch eta,  9.834778811104935e-05
LOSS train 0.31563 valid 0.29217, valid PER 9.47%
EPOCH 6:
isNotFrozen  True
  batch 50 loss: 0.2882015666365623
  batch 100 loss: 0.2744422098994255
  batch 150 loss: 0.26620210736989974
  batch 200 loss: 0.28079010397195814
  batch 250 loss: 0.3200464615225792
  batch 300 loss: 0.28761778354644774
  batch 350 loss: 0.28314189344644547
  batch 400 loss: 0.2572805100679398
  batch 450 loss: 0.2632069501280785
  batch 500 loss: 0.26958231657743453
  batch 550 loss: 0.2787570072710514
  batch 600 loss: 0.2728119836747646
  batch 650 loss: 0.27216767251491547
  batch 700 loss: 0.26844293385744095
  batch 750 loss: 0.2982091997563839
  batch 800 loss: 0.3077193273603916
  batch 850 loss: 0.273654069006443
  batch 900 loss: 0.2759972879290581
last epoch eta,  9.801199767883745e-05
LOSS train 0.27600 valid 0.29642, valid PER 8.47%
EPOCH 7:
isNotFrozen  True
  batch 50 loss: 0.2273044529557228
  batch 100 loss: 0.22626709878444673
  batch 150 loss: 0.2429300406575203
  batch 200 loss: 0.2559334635734558
  batch 250 loss: 0.2294877675175667
  batch 300 loss: 0.2520112121105194
  batch 350 loss: 0.2673223689198494
  batch 400 loss: 0.27245218604803084
  batch 450 loss: 0.27642840921878814
  batch 500 loss: 0.25803045809268954
  batch 550 loss: 0.2685287843644619
  batch 600 loss: 0.2597359138727188
  batch 650 loss: 0.2717015418410301
  batch 700 loss: 0.2479572957754135
  batch 750 loss: 0.2508248381316662
  batch 800 loss: 0.2735510104894638
  batch 850 loss: 0.2735898569226265
  batch 900 loss: 0.2558538997173309
last epoch eta,  9.767962340949277e-05
LOSS train 0.25585 valid 0.30130, valid PER 8.74%
EPOCH 8:
isNotFrozen  True
  batch 50 loss: 0.21619234710931778
  batch 100 loss: 0.23700667694211006
  batch 150 loss: 0.2286246371269226
  batch 200 loss: 0.2445116111636162
  batch 250 loss: 0.24293941721320153
  batch 300 loss: 0.22505919456481935
  batch 350 loss: 0.2418424054980278
  batch 400 loss: 0.24428705334663392
  batch 450 loss: 0.24850949615240098
  batch 500 loss: 0.24426325753331185
  batch 550 loss: 0.23413223654031753
  batch 600 loss: 0.22944291532039643
  batch 650 loss: 0.23604703322052956
  batch 700 loss: 0.2413273948431015
  batch 750 loss: 0.23863643497228623
  batch 800 loss: 0.2606647326052189
  batch 850 loss: 0.24287523001432418
  batch 900 loss: 0.24833228886127473
last epoch eta,  9.735060776924916e-05
LOSS train 0.24833 valid 0.31236, valid PER 8.91%
EPOCH 9:
isNotFrozen  True
  batch 50 loss: 0.20648699134588241
  batch 100 loss: 0.22144035816192628
  batch 150 loss: 0.22357576191425324
  batch 200 loss: 0.21296513572335243
  batch 250 loss: 0.22742972418665885
  batch 300 loss: 0.21271478578448297
  batch 350 loss: 0.217351266592741
  batch 400 loss: 0.22092636704444885
  batch 450 loss: 0.20748095825314522
  batch 500 loss: 0.237826229929924
  batch 550 loss: 0.24407061234116553
  batch 600 loss: 0.2394442716240883
  batch 650 loss: 0.23747316777706146
  batch 700 loss: 0.26052021980285645
  batch 750 loss: 0.2067028108239174
  batch 800 loss: 0.25650205194950104
  batch 850 loss: 0.22640630453824998
  batch 900 loss: 0.22842687755823135
last epoch eta,  9.70248945718133e-05
LOSS train 0.22843 valid 0.33579, valid PER 9.29%
EPOCH 10:
isNotFrozen  True
  batch 50 loss: 0.1977363023161888
  batch 100 loss: 0.1967686092853546
  batch 150 loss: 0.20928111873567104
  batch 200 loss: 0.20973906591534613
  batch 250 loss: 0.21148703172802924
  batch 300 loss: 0.1878850942850113
  batch 350 loss: 0.20133466079831122
  batch 400 loss: 0.22615692004561425
  batch 450 loss: 0.2121156221628189
  batch 500 loss: 0.1983130869269371
  batch 550 loss: 0.2285330720245838
  batch 600 loss: 0.19906570196151732
  batch 650 loss: 0.21092450208961963
  batch 700 loss: 0.21434703052043916
  batch 750 loss: 0.21456704631447793
  batch 800 loss: 0.2338291275501251
  batch 850 loss: 0.2150268067419529
  batch 900 loss: 0.21226025894284248
last epoch eta,  9.670242893805898e-05
LOSS train 0.21226 valid 0.34240, valid PER 9.03%
Training finished in 12.0 minutes.
Model saved to checkpoints/20240207_093003/model_5
Loading model from checkpoints/20240207_093003/model_5
CLEAN
 SUB: 5.49%, DEL: 3.16%, INS: 2.38%, COR: 91.35%, PER: 11.02%

