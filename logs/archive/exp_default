Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='adam', schedule_lr=True, freeze_layers=-1, inter_rep=0, combine_reps=False, warmup=False)
Total number of model parameters is 94402472
TRAIN LOADER LENGTH/NUMBER OF STEPS,  924
total_steps 9240, first_milestone 4620.0, second_milestone 4620.0
EPOCH 1:
isNotFrozen  False
  batch 50 loss: 11.147407932281494
  batch 100 loss: 8.91561902999878
  batch 150 loss: 6.815152559280396
  batch 200 loss: 5.06844277381897
  batch 250 loss: 4.2206802606582645
  batch 300 loss: 3.816137890815735
  batch 350 loss: 3.6124746084213255
  batch 400 loss: 3.539283580780029
  batch 450 loss: 3.4196763849258422
  batch 500 loss: 3.384931626319885
  batch 550 loss: 3.370340085029602
  batch 600 loss: 3.321522650718689
  batch 650 loss: 3.3143694972991944
  batch 700 loss: 3.3421199035644533
  batch 750 loss: 3.299151659011841
  batch 800 loss: 3.304767565727234
  batch 850 loss: 3.2965987396240233
  batch 900 loss: 3.2776614141464235
last epoch eta,  [0.0001]
LOSS train 3.27766 valid 3.47214, valid PER 99.91%
EPOCH 2:
isNotFrozen  False
  batch 50 loss: 3.243131899833679
  batch 100 loss: 3.2485008430480957
  batch 150 loss: 3.2493365097045896
  batch 200 loss: 3.219104542732239
  batch 250 loss: 3.2384375953674316
  batch 300 loss: 3.2369584465026855
  batch 350 loss: 3.218005862236023
  batch 400 loss: 3.2078265571594238
  batch 450 loss: 3.1877535009384155
  batch 500 loss: 3.1966661167144776
  batch 550 loss: 3.1787107801437378
  batch 600 loss: 3.163495306968689
  batch 650 loss: 3.1569665336608885
  batch 700 loss: 3.143667335510254
  batch 750 loss: 3.1516838026046754
  batch 800 loss: 3.1198856735229494
  batch 850 loss: 3.1139386796951296
  batch 900 loss: 3.1187612533569338
last epoch eta,  [0.0001]
LOSS train 3.11876 valid 3.21182, valid PER 98.60%
EPOCH 3:
isNotFrozen  False
  batch 50 loss: 3.1054778718948364
  batch 100 loss: 3.0757355165481566
  batch 150 loss: 3.0763760137557985
  batch 200 loss: 3.0569997453689575
  batch 250 loss: 3.011670184135437
  batch 300 loss: 3.0213445520401
  batch 350 loss: 3.0296374225616454
  batch 400 loss: 3.00517569065094
  batch 450 loss: 2.998829174041748
  batch 500 loss: 2.9575558280944825
  batch 550 loss: 2.929101676940918
  batch 600 loss: 2.900246915817261
  batch 650 loss: 2.8603448629379273
  batch 700 loss: 2.8576542234420774
  batch 750 loss: 2.851449203491211
  batch 800 loss: 2.8170650720596315
  batch 850 loss: 2.822496953010559
  batch 900 loss: 2.7893456411361695
last epoch eta,  [0.0001]
LOSS train 2.78935 valid 2.80971, valid PER 82.48%
EPOCH 4:
isNotFrozen  True
  batch 50 loss: 1.9354936122894286
  batch 100 loss: 1.1520645081996919
  batch 150 loss: 0.8161990511417389
  batch 200 loss: 0.7693301355838775
  batch 250 loss: 0.6934075111150741
  batch 300 loss: 0.6409177899360656
  batch 350 loss: 0.6009988474845886
  batch 400 loss: 0.6035687166452408
  batch 450 loss: 0.5415685242414474
  batch 500 loss: 0.5235448509454728
  batch 550 loss: 0.5918228900432587
  batch 600 loss: 0.5488630133867264
  batch 650 loss: 0.5246334075927734
  batch 700 loss: 0.505913976430893
  batch 750 loss: 0.48585603982210157
  batch 800 loss: 0.5092034259438515
  batch 850 loss: 0.50395886272192
  batch 900 loss: 0.49745521545410154
last epoch eta,  [0.0001]
LOSS train 0.49746 valid 0.32917, valid PER 9.51%
EPOCH 5:
isNotFrozen  True
  batch 50 loss: 0.4058273380994797
  batch 100 loss: 0.43471516132354737
  batch 150 loss: 0.43251169979572296
  batch 200 loss: 0.44217018723487855
  batch 250 loss: 0.460020996928215
  batch 300 loss: 0.44299010932445526
  batch 350 loss: 0.44437332570552823
  batch 400 loss: 0.44431840419769286
  batch 450 loss: 0.4133142253756523
  batch 500 loss: 0.4702280902862549
  batch 550 loss: 0.431016389131546
  batch 600 loss: 0.44380471497774127
  batch 650 loss: 0.4083771362900734
  batch 700 loss: 0.45363747864961623
  batch 750 loss: 0.39512128531932833
  batch 800 loss: 0.5875537395477295
  batch 850 loss: 0.49438015520572665
  batch 900 loss: 0.4991000729799271
last epoch eta,  [0.0001]
LOSS train 0.49910 valid 0.31337, valid PER 9.83%
EPOCH 6:
isNotFrozen  True
  batch 50 loss: 0.37798986345529556
  batch 100 loss: 0.352042955160141
  batch 150 loss: 0.34258921712636947
  batch 200 loss: 0.3712165758013725
  batch 250 loss: 0.3930591091513634
  batch 300 loss: 0.3747737681865692
  batch 350 loss: 0.3726098197698593
  batch 400 loss: 0.36397998839616774
  batch 450 loss: 0.38241264045238493
  batch 500 loss: 0.3985574498772621
  batch 550 loss: 0.3394842532277107
  batch 600 loss: 0.3284985858201981
  batch 650 loss: 0.350411434173584
  batch 700 loss: 0.34129963964223864
  batch 750 loss: 0.3603016957640648
  batch 800 loss: 0.3473171067237854
  batch 850 loss: 0.3440764367580414
  batch 900 loss: 0.38052974253892896
last epoch eta,  [7.999999999999974e-05]
LOSS train 0.38053 valid 0.32205, valid PER 9.84%
EPOCH 7:
isNotFrozen  True
  batch 50 loss: 0.32431161403656006
  batch 100 loss: 0.3163851174712181
  batch 150 loss: 0.30958402156829834
  batch 200 loss: 0.3325572282075882
  batch 250 loss: 0.2795375195145607
  batch 300 loss: 0.3151760180294514
  batch 350 loss: 0.3123805421590805
  batch 400 loss: 0.3259534195065498
  batch 450 loss: 0.3029366087913513
  batch 500 loss: 0.3011847361922264
  batch 550 loss: 0.3250622308254242
  batch 600 loss: 0.2891868487000465
  batch 650 loss: 0.4096215632557869
  batch 700 loss: 0.3525123715400696
  batch 750 loss: 0.3180717381834984
  batch 800 loss: 0.3227216121554375
  batch 850 loss: 0.3125337591767311
  batch 900 loss: 0.32674002289772036
last epoch eta,  [6.0000000000000123e-05]
LOSS train 0.32674 valid 0.29464, valid PER 9.37%
EPOCH 8:
isNotFrozen  True
  batch 50 loss: 0.2600347101688385
  batch 100 loss: 0.2705863347649574
  batch 150 loss: 0.27076705247163774
  batch 200 loss: 0.2608248895406723
  batch 250 loss: 0.25647530108690264
  batch 300 loss: 0.22932611852884294
  batch 350 loss: 0.28130236238241196
  batch 400 loss: 0.24526175662875174
  batch 450 loss: 0.26100000604987145
  batch 500 loss: 0.23540651112794875
  batch 550 loss: 0.2527562914788723
  batch 600 loss: 0.24408103078603743
  batch 650 loss: 0.23452265471220016
  batch 700 loss: 0.24646187335252762
  batch 750 loss: 0.23360797598958016
  batch 800 loss: 0.2431415283679962
  batch 850 loss: 0.2458152364194393
  batch 900 loss: 0.24080226436257363
last epoch eta,  [3.999999999999979e-05]
LOSS train 0.24080 valid 0.25848, valid PER 8.19%
EPOCH 9:
isNotFrozen  True
  batch 50 loss: 0.2152247905731201
  batch 100 loss: 0.21907658711075784
  batch 150 loss: 0.21199176251888274
  batch 200 loss: 0.19550250664353372
  batch 250 loss: 0.22701603442430496
  batch 300 loss: 0.21045784920454025
  batch 350 loss: 0.2068960689008236
  batch 400 loss: 0.21890137135982513
  batch 450 loss: 0.20082029610872268
  batch 500 loss: 0.23392156571149825
  batch 550 loss: 0.21416945591568948
  batch 600 loss: 0.214076739102602
  batch 650 loss: 0.21368562906980515
  batch 700 loss: 0.21277822837233543
  batch 750 loss: 0.19093323096632958
  batch 800 loss: 0.20662658989429475
  batch 850 loss: 0.2098549436032772
  batch 900 loss: 0.1796184140443802
last epoch eta,  [1.9999999999999653e-05]
LOSS train 0.17962 valid 0.25788, valid PER 7.75%
EPOCH 10:
isNotFrozen  True
  batch 50 loss: 0.1836664643883705
  batch 100 loss: 0.18542599454522132
  batch 150 loss: 0.17501211076974868
  batch 200 loss: 0.17389691650867461
  batch 250 loss: 0.17875682234764098
  batch 300 loss: 0.1759849001467228
  batch 350 loss: 0.16137298986315726
  batch 400 loss: 0.16512906834483146
  batch 450 loss: 0.17545147478580475
  batch 500 loss: 0.18104571834206581
  batch 550 loss: 0.1832011778652668
  batch 600 loss: 0.15470822304487228
  batch 650 loss: 0.16525789052248002
  batch 700 loss: 0.16799019441008567
  batch 750 loss: 0.17565170086920262
  batch 800 loss: 0.16204677678644658
  batch 850 loss: 0.16791927501559256
  batch 900 loss: 0.1608213007450104
last epoch eta,  [0.0]
LOSS train 0.16082 valid 0.26676, valid PER 7.85%
Training finished in 10.0 minutes.
Model saved to checkpoints/20240206_220931/model_9
Loading model from checkpoints/20240206_220931/model_9
CLEAN
 SUB: 5.28%, DEL: 1.93%, INS: 2.24%, COR: 92.78%, PER: 9.45%

