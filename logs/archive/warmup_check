Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=1, optimiser='adam', schedule_lr=True, freeze_layers=0, inter_rep=0, combine_reps=False, warmup=True)
Total number of model parameters is 94402472
TRAIN LOADER LENGTH/NUMBER OF STEPS,  924
warmup_milestone 92.4
total_steps 924, first_milestone 369.6, second_milestone 554.4
EPOCH 1:
isNotFrozen  True
  batch 50 loss: 6.864813423156738
  batch 100 loss: 3.355146484375
  batch 150 loss: 3.314147081375122
  batch 200 loss: 3.3081529092788697
  batch 250 loss: 3.2966442680358887
  batch 300 loss: 3.2658330965042115
  batch 350 loss: 3.2146369552612306
  batch 400 loss: 3.0441931295394897
  batch 450 loss: 2.5342968606948855
  batch 500 loss: 1.8478085327148437
  batch 550 loss: 1.4101078248023986
  batch 600 loss: 1.1340302324295044
  batch 650 loss: 0.9417977964878083
  batch 700 loss: 0.8831674730777741
  batch 750 loss: 0.7643446540832519
  batch 800 loss: 0.687515361905098
  batch 850 loss: 0.6770572036504745
  batch 900 loss: 0.6409738028049469
first 5 warmup etas, [[1.092142857142857e-06], [2.174285714285714e-06], [3.256428571428571e-06], [4.338571428571428e-06], [5.420714285714286e-06]], last 5 warmup etas, [[9.956714285714283e-05], [9.956714285714283e-05], [9.956714285714283e-05], [9.956714285714283e-05], [9.956714285714283e-05]]
last epoch eta,  [9.956714285714283e-05]
LOSS train 0.64097 valid 0.45694, valid PER 11.67%
Training finished in 1.0 minutes.
Model saved to checkpoints/20240206_230846/model_1
Loading model from checkpoints/20240206_230846/model_1
CLEAN
 SUB: 7.15%, DEL: 4.16%, INS: 2.22%, COR: 88.70%, PER: 13.53%

