Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='noam', schedule_lr=False, freeze_layers=0, inter_rep=0, combine_reps=False, warmup=False)
Total number of model parameters is 94402472
EPOCH 1:
isNotFrozen  True
  batch 50 loss: 10.478985042572022
  batch 100 loss: 4.6253742456436155
  batch 150 loss: 3.456957154273987
  batch 200 loss: 3.3511445331573486
  batch 250 loss: 3.3121938705444336
  batch 300 loss: 3.266291003227234
  batch 350 loss: 3.2357356548309326
  batch 400 loss: 3.196110029220581
  batch 450 loss: 3.05908812046051
  batch 500 loss: 2.7721914434432984
  batch 550 loss: 2.348315215110779
  batch 600 loss: 1.803465542793274
  batch 650 loss: 1.3242808675765991
  batch 700 loss: 1.019962214231491
  batch 750 loss: 0.785447187423706
  batch 800 loss: 0.6848166370391846
  batch 850 loss: 0.6435451507568359
  batch 900 loss: 0.6217148280143738
last epoch eta,  0.0001317957403996806
LOSS train 0.62171 valid 0.44057, valid PER 11.95%
EPOCH 2:
isNotFrozen  True
  batch 50 loss: 0.5786022520065308
  batch 100 loss: 0.6126946753263474
  batch 150 loss: 0.5714974063634872
  batch 200 loss: 0.5555109906196595
  batch 250 loss: 0.6351486414670944
  batch 300 loss: 0.5862961250543595
  batch 350 loss: 0.5946656966209412
  batch 400 loss: 0.5561155492067337
  batch 450 loss: 0.5626546990871429
  batch 500 loss: 0.5939503306150437
  batch 550 loss: 0.6515800601243973
  batch 600 loss: 0.6082609611749649
  batch 650 loss: 0.6154723125696182
  batch 700 loss: 0.6665106076002121
  batch 750 loss: 0.634667522907257
  batch 800 loss: 0.6114452517032624
  batch 850 loss: 0.6112390178442001
  batch 900 loss: 0.715974999666214
last epoch eta,  0.0002635914807993612
LOSS train 0.71597 valid 0.54278, valid PER 15.08%
EPOCH 3:
isNotFrozen  True
  batch 50 loss: 0.6315677064657211
  batch 100 loss: 0.6863952165842057
  batch 150 loss: 0.7668688797950745
  batch 200 loss: 0.6721541064977646
  batch 250 loss: 0.6152365988492966
  batch 300 loss: 0.6700248754024506
  batch 350 loss: 0.6714366137981415
  batch 400 loss: 0.6523399287462235
  batch 450 loss: 0.6917518508434296
  batch 500 loss: 0.6877940970659256
  batch 550 loss: 0.6956504392623901
  batch 600 loss: 0.739385312795639
  batch 650 loss: 0.697086940407753
  batch 700 loss: 0.7471330714225769
  batch 750 loss: 0.7550745922327041
  batch 800 loss: 0.857784606218338
  batch 850 loss: 4.019041142463684
  batch 900 loss: 3.299661331176758
last epoch eta,  0.00039538722119904175
LOSS train 3.29966 valid 3.31376, valid PER 100.00%
EPOCH 4:
isNotFrozen  True
  batch 50 loss: 3.3024433517456053
  batch 100 loss: 3.3012849426269533
  batch 150 loss: 3.2863835525512695
  batch 200 loss: 3.3184756422042847
  batch 250 loss: 3.3233653688430786
  batch 300 loss: 3.3172482204437257
  batch 350 loss: 3.2904181623458864
  batch 400 loss: 3.304396643638611
  batch 450 loss: 3.3167863082885742
  batch 500 loss: 3.284563412666321
  batch 550 loss: 3.3175963163375854
  batch 600 loss: 3.314436340332031
  batch 650 loss: 3.3081886911392213
  batch 700 loss: 3.3118600368499758
  batch 750 loss: 3.2856629133224486
  batch 800 loss: 3.29148323059082
  batch 850 loss: 3.2883448505401613
  batch 900 loss: 3.3447102642059328
last epoch eta,  0.0005271829615987224
LOSS train 3.34471 valid 3.31349, valid PER 100.00%
EPOCH 5:
isNotFrozen  True
  batch 50 loss: 3.2881990575790407
  batch 100 loss: 3.292660059928894
  batch 150 loss: 3.280962462425232
  batch 200 loss: 3.2977364301681518
  batch 250 loss: 3.3020320320129395
  batch 300 loss: 3.2957845640182497
  batch 350 loss: 3.3097069215774537
  batch 400 loss: 3.3075639629364013
  batch 450 loss: 3.2898763847351074
  batch 500 loss: 3.310125346183777
  batch 550 loss: 3.2983184671401977
  batch 600 loss: 3.2908723735809327
  batch 650 loss: 3.2953313255310057
  batch 700 loss: 3.3038293170928954
  batch 750 loss: 3.3002788972854615
  batch 800 loss: 3.314842014312744
  batch 850 loss: 3.3105056476593018
  batch 900 loss: 3.3074807596206663
last epoch eta,  0.0005308824910808599
LOSS train 3.30748 valid 3.31937, valid PER 100.00%
EPOCH 6:
isNotFrozen  True
  batch 50 loss: 3.2839567041397095
  batch 100 loss: 3.3109800148010256
  batch 150 loss: 3.29248101234436
  batch 200 loss: 3.302234001159668
  batch 250 loss: 3.306765441894531
  batch 300 loss: 3.2778133869171144
  batch 350 loss: 3.3006310081481933
  batch 400 loss: 3.2886578750610354
  batch 450 loss: 3.3021975421905516
  batch 500 loss: 3.286342434883118
  batch 550 loss: 3.293424062728882
  batch 600 loss: 3.288803367614746
  batch 650 loss: 3.3037098264694214
  batch 700 loss: 3.3045436334609986
  batch 750 loss: 3.3009412717819213
  batch 800 loss: 3.2900736284255983
  batch 850 loss: 3.303528823852539
  batch 900 loss: 3.304960837364197
last epoch eta,  0.0004846271929158702
LOSS train 3.30496 valid 3.32854, valid PER 100.00%
EPOCH 7:
isNotFrozen  True
  batch 50 loss: 3.2947955465316774
  batch 100 loss: 3.3079209423065183
  batch 150 loss: 3.288645477294922
  batch 200 loss: 3.3007555389404297
  batch 250 loss: 3.3002439403533934
