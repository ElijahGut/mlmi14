Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='noam', schedule_lr=False, freeze_layers=0, inter_rep=0, combine_reps=False, warmup=False)
Total number of model parameters is 94402472
EPOCH 1:
isNotFrozen  True
  batch 50 loss: 11.63573112487793
  batch 100 loss: 8.350868520736695
  batch 150 loss: 4.429625029563904
  batch 200 loss: 3.5718225717544554
  batch 250 loss: 3.414819974899292
  batch 300 loss: 3.3471338987350463
  batch 350 loss: 3.3138409566879274
  batch 400 loss: 3.3033101558685303
  batch 450 loss: 3.265930871963501
  batch 500 loss: 3.22081666469574
  batch 550 loss: 3.163437943458557
  batch 600 loss: 3.114263696670532
  batch 650 loss: 2.9777716064453124
  batch 700 loss: 2.839421858787537
  batch 750 loss: 2.605041651725769
  batch 800 loss: 2.3318142080307007
  batch 850 loss: 1.9733420872688294
  batch 900 loss: 1.6417159867286681
last epoch eta,  4.5851281589913054e-05
LOSS train 1.64172 valid 1.20677, valid PER 31.97%
EPOCH 2:
isNotFrozen  True
  batch 50 loss: 1.2017300641536712
  batch 100 loss: 0.9718685340881348
  batch 150 loss: 0.8062275242805481
  batch 200 loss: 0.7397471404075623
  batch 250 loss: 0.6905986511707306
  batch 300 loss: 0.6213822931051254
  batch 350 loss: 0.5962299925088882
  batch 400 loss: 0.5711181485652923
  batch 450 loss: 0.5086655759811402
  batch 500 loss: 0.549898282289505
  batch 550 loss: 0.5689808052778244
  batch 600 loss: 0.5036372947692871
  batch 650 loss: 0.5044321006536484
  batch 700 loss: 0.511196704506874
  batch 750 loss: 0.5110640034079552
  batch 800 loss: 0.4711807072162628
  batch 850 loss: 0.5029117202758789
  batch 900 loss: 0.4686058658361435
last epoch eta,  9.170256317982611e-05
LOSS train 0.46861 valid 0.35109, valid PER 9.91%
EPOCH 3:
isNotFrozen  True
  batch 50 loss: 0.4387577646970749
  batch 100 loss: 0.4433205366134644
  batch 150 loss: 0.4288346463441849
  batch 200 loss: 0.4250921025872231
  batch 250 loss: 0.4495254594087601
  batch 300 loss: 0.44670227736234663
  batch 350 loss: 0.46575092792510986
  batch 400 loss: 0.4250921005010605
  batch 450 loss: 0.4217454731464386
  batch 500 loss: 0.4724435955286026
  batch 550 loss: 0.4603814661502838
  batch 600 loss: 0.39004586219787596
  batch 650 loss: 0.39050276935100553
  batch 700 loss: 0.43178248405456543
  batch 750 loss: 0.44601729601621626
  batch 800 loss: 0.39263969630002976
  batch 850 loss: 0.4190192928910255
  batch 900 loss: 0.37937226265668866
last epoch eta,  8.43000609167204e-05
LOSS train 0.37937 valid 0.31269, valid PER 9.43%
EPOCH 4:
isNotFrozen  True
  batch 50 loss: 0.3763379034399986
  batch 100 loss: 0.38054971903562546
  batch 150 loss: 0.33687309205532073
  batch 200 loss: 0.34771606743335726
  batch 250 loss: 0.340781002342701
  batch 300 loss: 0.3405827933549881
  batch 350 loss: 0.3334075021743774
  batch 400 loss: 0.3773292005062103
  batch 450 loss: 0.33688866764307024
  batch 500 loss: 0.3382572245597839
  batch 550 loss: 0.3592159488797188
  batch 600 loss: 0.3567456325888634
  batch 650 loss: 0.31546453446149825
  batch 700 loss: 0.3339565870165825
  batch 750 loss: 0.32796337515115737
  batch 800 loss: 0.32801390916109086
  batch 850 loss: 0.34584675371646884
  batch 900 loss: 0.3497106483578682
last epoch eta,  7.300599429445556e-05
LOSS train 0.34971 valid 0.29102, valid PER 8.53%
EPOCH 5:
isNotFrozen  True
  batch 50 loss: 0.2634077656269074
  batch 100 loss: 0.28496209025382996
  batch 150 loss: 0.3235486781597137
  batch 200 loss: 0.28886468574404717
  batch 250 loss: 0.29733850598335265
  batch 300 loss: 0.3111009231209755
  batch 350 loss: 0.2731875444948673
  batch 400 loss: 0.28137446343898775
  batch 450 loss: 0.3003490400314331
  batch 500 loss: 0.3145060008764267
  batch 550 loss: 0.2738843157887459
  batch 600 loss: 0.2867516231536865
  batch 650 loss: 0.29283877775073053
  batch 700 loss: 0.3104530316591263
  batch 750 loss: 0.276037919819355
  batch 800 loss: 0.2842642942070961
  batch 850 loss: 0.28470328271389006
  batch 900 loss: 0.3023462963104248
last epoch eta,  6.529854640294577e-05
LOSS train 0.30235 valid 0.27413, valid PER 8.46%
EPOCH 6:
isNotFrozen  True
  batch 50 loss: 0.24506793320178985
  batch 100 loss: 0.23836999297142028
  batch 150 loss: 0.2418324215710163
  batch 200 loss: 0.25549677982926366
  batch 250 loss: 0.2427324701845646
  batch 300 loss: 0.2648631012439728
  batch 350 loss: 0.24970363959670067
  batch 400 loss: 0.23997337132692337
  batch 450 loss: 0.24684685170650483
  batch 500 loss: 0.2564446514844894
  batch 550 loss: 0.23938076823949814
  batch 600 loss: 0.25767575174570084
  batch 650 loss: 0.24773205041885377
  batch 700 loss: 0.26650699228048325
  batch 750 loss: 0.277448992729187
  batch 800 loss: 0.26472465217113494
  batch 850 loss: 0.2604690772294998
  batch 900 loss: 0.26007080733776095
last epoch eta,  5.9609144728652034e-05
LOSS train 0.26007 valid 0.30468, valid PER 8.47%
EPOCH 7:
isNotFrozen  True
  batch 50 loss: 0.21544989064335823
  batch 100 loss: 0.23106517851352693
  batch 150 loss: 0.22976068764925003
  batch 200 loss: 0.22025764510035514
  batch 250 loss: 0.21248461455106735
  batch 300 loss: 0.22295196995139122
  batch 350 loss: 0.23330361947417258
  batch 400 loss: 0.23539736568927766
  batch 450 loss: 0.2173706153035164
  batch 500 loss: 0.215108160674572
  batch 550 loss: 0.21593910604715347
  batch 600 loss: 0.22342582836747168
  batch 650 loss: 0.2327701587975025
  batch 700 loss: 0.22795192927122115
  batch 750 loss: 0.2094641862809658
  batch 800 loss: 0.2395402731001377
  batch 850 loss: 0.224139247238636
  batch 900 loss: 0.2243664325773716
last epoch eta,  5.518734432003709e-05
LOSS train 0.22437 valid 0.28145, valid PER 8.43%
EPOCH 8:
isNotFrozen  True
  batch 50 loss: 0.1731062676012516
  batch 100 loss: 0.20419211447238922
  batch 150 loss: 0.19375178694725037
  batch 200 loss: 0.18875779427587985
  batch 250 loss: 0.18806515038013458
  batch 300 loss: 0.19066050723195077
  batch 350 loss: 0.20243717327713967
  batch 400 loss: 0.19583878189325332
  batch 450 loss: 0.2038528084754944
  batch 500 loss: 0.20519219413399697
  batch 550 loss: 0.20603789195418357
  batch 600 loss: 0.20015828371047972
  batch 650 loss: 0.20016444236040115
  batch 700 loss: 0.20001789614558219
  batch 750 loss: 0.19935168370604514
  batch 800 loss: 0.20619701102375984
  batch 850 loss: 0.20304476618766784
  batch 900 loss: 0.21392841666936874
last epoch eta,  5.162303363287592e-05
LOSS train 0.21393 valid 0.29605, valid PER 8.52%
EPOCH 9:
isNotFrozen  True
  batch 50 loss: 0.19284659065306187
  batch 100 loss: 0.18792628422379493
  batch 150 loss: 0.18087643899023534
  batch 200 loss: 0.1932605142146349
  batch 250 loss: 0.18622903659939766
  batch 300 loss: 0.17901593163609505
  batch 350 loss: 0.1924468320608139
  batch 400 loss: 0.17258333623409272
  batch 450 loss: 0.17761623874306678
  batch 500 loss: 0.17392787143588065
  batch 550 loss: 0.19776459485292436
  batch 600 loss: 0.19181808203458786
  batch 650 loss: 0.18559747397899629
  batch 700 loss: 0.18048084288835525
  batch 750 loss: 0.1828571528196335
  batch 800 loss: 0.18557893007993698
  batch 850 loss: 0.1810575780272484
  batch 900 loss: 0.16134518325328828
last epoch eta,  4.8670662862970364e-05
LOSS train 0.16135 valid 0.31541, valid PER 8.39%
EPOCH 10:
isNotFrozen  True
  batch 50 loss: 0.16316416040062903
  batch 100 loss: 0.1560239592939615
  batch 150 loss: 0.1662433071434498
  batch 200 loss: 0.1536077618598938
  batch 250 loss: 0.1663802121579647
  batch 300 loss: 0.17920756042003633
  batch 350 loss: 0.15209758915007116
  batch 400 loss: 0.15547971293330193
  batch 450 loss: 0.16105016134679317
  batch 500 loss: 0.16771437719464302
  batch 550 loss: 0.16488687679171563
  batch 600 loss: 0.15544820107519627
  batch 650 loss: 0.16120170876383783
  batch 700 loss: 0.15725796937942504
  batch 750 loss: 0.15216278418898582
  batch 800 loss: 0.17942765705287456
  batch 850 loss: 0.17598036035895348
  batch 900 loss: 0.15544673144817353
last epoch eta,  4.617304496314739e-05
LOSS train 0.15545 valid 0.31002, valid PER 8.02%
Training finished in 14.0 minutes.
Model saved to checkpoints/20240207_174533/model_5
Loading model from checkpoints/20240207_174533/model_5
CLEAN
 SUB: 5.51%, DEL: 2.28%, INS: 2.39%, COR: 92.21%, PER: 10.18%

