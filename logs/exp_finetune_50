Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='adam', schedule_lr=True, freeze_layers=-1, first_milestone=0.4)
Total number of model parameters is 94402472
Adjusting learning rate of group 0 to 1.0000e-04.
Adjusting learning rate of group 0 to 1.0000e-04.
EPOCH 1:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 10.743069839477538
  batch 100 loss: 8.29122281074524
  batch 150 loss: 6.294808187484741
  batch 200 loss: 4.822299613952636
  batch 250 loss: 4.087126755714417
  batch 300 loss: 3.768170404434204
  batch 350 loss: 3.5901233100891115
  batch 400 loss: 3.517875814437866
  batch 450 loss: 3.4315162086486817
  batch 500 loss: 3.389562044143677
  batch 550 loss: 3.3556706285476685
  batch 600 loss: 3.326306700706482
  batch 650 loss: 3.316608052253723
  batch 700 loss: 3.3151463365554807
  batch 750 loss: 3.291014609336853
  batch 800 loss: 3.2929842376708987
  batch 850 loss: 3.2884758281707764
  batch 900 loss: 3.2616435194015505
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.26164 valid 3.46627, valid PER 100.00%
EPOCH 2:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.249081311225891
  batch 100 loss: 3.2345026254653932
  batch 150 loss: 3.240307025909424
  batch 200 loss: 3.2160046100616455
  batch 250 loss: 3.225082287788391
  batch 300 loss: 3.2134550762176515
  batch 350 loss: 3.197707591056824
  batch 400 loss: 3.1988978004455566
  batch 450 loss: 3.178399691581726
  batch 500 loss: 3.205203104019165
  batch 550 loss: 3.17559663772583
  batch 600 loss: 3.1531665086746217
  batch 650 loss: 3.152127661705017
  batch 700 loss: 3.1369985437393186
  batch 750 loss: 3.1380953216552734
  batch 800 loss: 3.104554991722107
  batch 850 loss: 3.125316662788391
  batch 900 loss: 3.097671389579773
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.09767 valid 3.18057, valid PER 97.24%
EPOCH 3:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.091182928085327
  batch 100 loss: 3.0520319366455078
  batch 150 loss: 3.055763964653015
  batch 200 loss: 3.0480070066452027
  batch 250 loss: 3.008603420257568
  batch 300 loss: 3.0156735038757323
  batch 350 loss: 3.0158463430404665
  batch 400 loss: 2.9711091184616087
  batch 450 loss: 2.980340828895569
  batch 500 loss: 2.9472175884246825
  batch 550 loss: 2.9287567806243895
  batch 600 loss: 2.906939764022827
  batch 650 loss: 2.8477168703079223
  batch 700 loss: 2.838692750930786
  batch 750 loss: 2.8704248237609864
  batch 800 loss: 2.8252063179016114
  batch 850 loss: 2.8172666692733763
  batch 900 loss: 2.778904161453247
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 2.77890 valid 2.79888, valid PER 82.12%
EPOCH 4:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 2.043696630001068
  batch 100 loss: 1.2000650191307067
  batch 150 loss: 0.8879925549030304
  batch 200 loss: 0.8386771404743194
  batch 250 loss: 0.668093023300171
  batch 300 loss: 0.6364777165651322
  batch 350 loss: 0.8041755187511445
  batch 400 loss: 0.5994518238306046
  batch 450 loss: 0.591360941529274
  batch 500 loss: 0.5510614114999771
  batch 550 loss: 0.5435278189182281
  batch 600 loss: 0.5574796116352081
  batch 650 loss: 0.5783115863800049
  batch 700 loss: 0.5786668753623962
  batch 750 loss: 0.5368371003866196
  batch 800 loss: 0.5109896805882453
  batch 850 loss: 0.5102088129520417
  batch 900 loss: 0.5083436232805252
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.50834 valid 0.34280, valid PER 9.69%
EPOCH 5:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.4070548412203789
  batch 100 loss: 0.4629600977897644
  batch 150 loss: 0.44567233383655547
  batch 200 loss: 0.4201343733072281
  batch 250 loss: 0.4495068973302841
  batch 300 loss: 0.44388513922691347
  batch 350 loss: 0.40282793283462526
  batch 400 loss: 0.41390649765729903
  batch 450 loss: 0.4170375144481659
  batch 500 loss: 0.43380872935056686
  batch 550 loss: 0.43353269159793856
  batch 600 loss: 0.44329367965459826
  batch 650 loss: 0.4186608025431633
  batch 700 loss: 0.4231929126381874
  batch 750 loss: 0.42693402856588364
  batch 800 loss: 0.47230368852615356
  batch 850 loss: 0.40562265843153
  batch 900 loss: 0.4367613446712494
Adjusting learning rate of group 0 to 8.3333e-05.
LOSS train 0.43676 valid 0.33414, valid PER 9.94%
EPOCH 6:
ETA [8.333333333333334e-05]

isNotFrozen  True
  batch 50 loss: 0.3573009720444679
  batch 100 loss: 0.35852895855903627
  batch 150 loss: 0.39095524430274964
  batch 200 loss: 0.3721619734168053
  batch 250 loss: 0.37098986983299254
  batch 300 loss: 0.3717872202396393
  batch 350 loss: 0.3699602672457695
  batch 400 loss: 0.34601287394762037
  batch 450 loss: 0.36419930040836335
  batch 500 loss: 0.3506526285409927
  batch 550 loss: 0.33308878272771836
  batch 600 loss: 0.33631361693143846
  batch 650 loss: 0.37277167171239856
  batch 700 loss: 0.4189394456148148
  batch 750 loss: 0.3948737236857414
  batch 800 loss: 0.43456772208213806
  batch 850 loss: 0.4054701405763626
  batch 900 loss: 0.4604619124531746
Adjusting learning rate of group 0 to 6.6667e-05.
LOSS train 0.46046 valid 0.34043, valid PER 10.67%
EPOCH 7:
ETA [6.666666666666668e-05]

isNotFrozen  True
  batch 50 loss: 0.3465153592824936
  batch 100 loss: 0.3488959056138992
  batch 150 loss: 0.32489662498235705
  batch 200 loss: 0.342515754699707
  batch 250 loss: 0.30509053319692614
  batch 300 loss: 0.30515636146068575
  batch 350 loss: 0.3417516788840294
  batch 400 loss: 0.3402149042487144
  batch 450 loss: 0.34487132996320724
  batch 500 loss: 0.33567471921443937
  batch 550 loss: 0.3318434488773346
  batch 600 loss: 0.3694763231277466
  batch 650 loss: 0.37288729816675187
  batch 700 loss: 0.3390051385760307
  batch 750 loss: 0.3109757819771767
  batch 800 loss: 0.3455474254488945
  batch 850 loss: 0.4389099186658859
  batch 900 loss: 0.37821379184722903
Adjusting learning rate of group 0 to 5.0000e-05.
LOSS train 0.37821 valid 0.32594, valid PER 9.62%
EPOCH 8:
ETA [5.000000000000001e-05]

isNotFrozen  True
  batch 50 loss: 0.3083325463533402
  batch 100 loss: 0.2935528963804245
  batch 150 loss: 0.32580032050609586
  batch 200 loss: 0.2757652007043362
  batch 250 loss: 0.2491944858431816
  batch 300 loss: 0.2624755918979645
  batch 350 loss: 0.30861564055085183
  batch 400 loss: 0.27650239154696465
  batch 450 loss: 0.2676346111297607
  batch 500 loss: 0.2596322047710419
  batch 550 loss: 0.26727723866701125
  batch 600 loss: 0.2656761062145233
  batch 650 loss: 0.2549705505371094
  batch 700 loss: 0.243911242634058
  batch 750 loss: 0.2274310879409313
  batch 800 loss: 0.2668904560804367
  batch 850 loss: 0.24718896940350532
  batch 900 loss: 0.2615459543466568
Adjusting learning rate of group 0 to 3.3333e-05.
LOSS train 0.26155 valid 0.28387, valid PER 8.45%
EPOCH 9:
ETA [3.333333333333334e-05]

isNotFrozen  True
  batch 50 loss: 0.24745837189257144
  batch 100 loss: 0.2205950327217579
  batch 150 loss: 0.21232293635606767
  batch 200 loss: 0.210002151876688
  batch 250 loss: 0.2556130966544151
  batch 300 loss: 0.22391400009393692
  batch 350 loss: 0.22458615005016327
  batch 400 loss: 0.2348706266283989
  batch 450 loss: 0.2398740166425705
  batch 500 loss: 0.21748421892523764
  batch 550 loss: 0.22301302351057528
  batch 600 loss: 0.2242567940056324
  batch 650 loss: 0.20034561797976494
  batch 700 loss: 0.22192202270030975
  batch 750 loss: 0.21697696834802627
  batch 800 loss: 0.21121186077594756
  batch 850 loss: 0.23266918629407882
  batch 900 loss: 0.2216689918935299
Adjusting learning rate of group 0 to 1.6667e-05.
LOSS train 0.22167 valid 0.28367, valid PER 8.32%
EPOCH 10:
ETA [1.666666666666667e-05]

isNotFrozen  True
  batch 50 loss: 0.18840556412935258
  batch 100 loss: 0.20028657481074333
  batch 150 loss: 0.1884855306148529
  batch 200 loss: 0.18503994300961493
  batch 250 loss: 0.19404620438814163
  batch 300 loss: 0.1818596540391445
  batch 350 loss: 0.19013609483838081
  batch 400 loss: 0.18399752527475358
  batch 450 loss: 0.1715996053814888
  batch 500 loss: 0.1915069602429867
  batch 550 loss: 0.19843029245734214
  batch 600 loss: 0.1763906043767929
  batch 650 loss: 0.19761338636279105
  batch 700 loss: 0.18550628706812858
  batch 750 loss: 0.19128687664866448
  batch 800 loss: 0.19100659891963004
  batch 850 loss: 0.17277748346328736
  batch 900 loss: 0.19339669048786162
Adjusting learning rate of group 0 to 0.0000e+00.
LOSS train 0.19340 valid 0.27844, valid PER 8.10%
Training finished in 12.0 minutes.
Model saved to checkpoints/20240201_090052/model_10
Loading model from checkpoints/20240201_090052/model_10
SUB: 5.05%, DEL: 1.96%, INS: 2.24%, COR: 92.99%, PER: 9.25%
