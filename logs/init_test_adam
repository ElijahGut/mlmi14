Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=20, optimiser='adam', schedule_lr=False, freeze_layers=0)
Total number of model parameters is 94402472
EPOCH 1:
isNotFrozen  True
  batch 50 loss: 3.9906874561309813
  batch 100 loss: 3.3076968240737914
  batch 150 loss: 3.308660378456116
  batch 200 loss: 3.3076667976379395
  batch 250 loss: 3.298808746337891
  batch 300 loss: 3.284488401412964
  batch 350 loss: 3.271298232078552
  batch 400 loss: 3.2662819147109987
  batch 450 loss: 3.249384183883667
  batch 500 loss: 3.202748012542725
  batch 550 loss: 3.1542932415008544
  batch 600 loss: 3.029643597602844
  batch 650 loss: 2.884445514678955
  batch 700 loss: 2.704259781837463
  batch 750 loss: 2.198694384098053
  batch 800 loss: 1.642513394355774
  batch 850 loss: 1.2016190087795258
  batch 900 loss: 0.9632440519332885
LOSS train 0.96324 valid 0.81846, valid PER 17.74%
EPOCH 2:
isNotFrozen  True
  batch 50 loss: 0.8091266775131225
  batch 100 loss: 0.7979381096363067
  batch 150 loss: 0.700196036696434
  batch 200 loss: 0.7053251016139984
  batch 250 loss: 0.743787779211998
  batch 300 loss: 0.8730291944742202
  batch 350 loss: 0.6343474590778351
  batch 400 loss: 0.6061864084005356
  batch 450 loss: 0.5457655608654022
  batch 500 loss: 0.5563840317726135
  batch 550 loss: 0.5497237533330918
  batch 600 loss: 0.5247181504964828
  batch 650 loss: 0.523665469288826
  batch 700 loss: 0.542892900109291
  batch 750 loss: 0.4787209153175354
  batch 800 loss: 0.46851730704307554
  batch 850 loss: 0.49575803577899935
  batch 900 loss: 0.4622247266769409
LOSS train 0.46222 valid 0.33522, valid PER 9.78%
EPOCH 3:
isNotFrozen  True
  batch 50 loss: 0.416553798019886
  batch 100 loss: 0.3863149204850197
  batch 150 loss: 0.38932951480150224
  batch 200 loss: 0.41365795373916625
  batch 250 loss: 0.4283048778772354
  batch 300 loss: 0.409444600045681
  batch 350 loss: 0.4223874098062515
  batch 400 loss: 0.39416305989027023
  batch 450 loss: 0.4107047826051712
  batch 500 loss: 0.39804976373910905
  batch 550 loss: 0.39954403042793274
  batch 600 loss: 0.3934984068572521
  batch 650 loss: 0.36752148658037187
  batch 700 loss: 0.3627985021471977
  batch 750 loss: 0.38820966869592666
  batch 800 loss: 0.41963605403900145
  batch 850 loss: 0.3956040444970131
  batch 900 loss: 0.3704493287205696
LOSS train 0.37045 valid 0.32332, valid PER 9.57%
EPOCH 4:
isNotFrozen  True
  batch 50 loss: 0.3512083613872528
  batch 100 loss: 0.34744151175022125
  batch 150 loss: 0.3262964181602001
  batch 200 loss: 0.3362337705492973
  batch 250 loss: 0.3434667101502418
  batch 300 loss: 0.32909685134887695
  batch 350 loss: 0.3278829291462898
  batch 400 loss: 0.3496498641371727
  batch 450 loss: 0.3685971537232399
  batch 500 loss: 0.33385596722364425
  batch 550 loss: 0.33411856934428213
  batch 600 loss: 0.38627818644046785
  batch 650 loss: 0.33778370141983033
  batch 700 loss: 0.3398179316520691
  batch 750 loss: 0.38007955223321915
  batch 800 loss: 0.33989781960844995
  batch 850 loss: 0.381616131067276
  batch 900 loss: 0.34405525743961335
LOSS train 0.34406 valid 0.30083, valid PER 9.18%
EPOCH 5:
isNotFrozen  True
  batch 50 loss: 0.3038051179051399
  batch 100 loss: 0.2744854339957237
  batch 150 loss: 0.3080927348136902
  batch 200 loss: 0.32102440029382706
  batch 250 loss: 0.3101764160394669
  batch 300 loss: 0.29270587384700775
  batch 350 loss: 0.30760629162192343
  batch 400 loss: 0.28906746178865433
  batch 450 loss: 0.29217427790164946
  batch 500 loss: 0.365361068546772
  batch 550 loss: 0.3350761094689369
  batch 600 loss: 0.3028814825415611
  batch 650 loss: 0.31510173618793486
  batch 700 loss: 0.3265117844939232
  batch 750 loss: 0.33138355642557143
  batch 800 loss: 0.33603374361991883
  batch 850 loss: 0.32196381971240046
  batch 900 loss: 0.34416587471961974
LOSS train 0.34417 valid 0.30271, valid PER 9.11%
EPOCH 6:
isNotFrozen  True
  batch 50 loss: 0.2961451482772827
  batch 100 loss: 0.28506531804800034
  batch 150 loss: 0.2806659629940987
  batch 200 loss: 0.2808788633346558
  batch 250 loss: 0.30613827526569365
  batch 300 loss: 0.3050776019692421
  batch 350 loss: 0.3052495360374451
  batch 400 loss: 0.27236399620771407
  batch 450 loss: 0.28188792496919635
  batch 500 loss: 0.2878016382455826
  batch 550 loss: 0.28497092619538306
  batch 600 loss: 0.3387662160396576
  batch 650 loss: 0.3021463802456856
  batch 700 loss: 0.3163102474808693
  batch 750 loss: 0.3312665671110153
  batch 800 loss: 0.31143509209156034
  batch 850 loss: 0.28297574907541273
  batch 900 loss: 0.33547641143202783
LOSS train 0.33548 valid 0.31440, valid PER 9.33%
EPOCH 7:
isNotFrozen  True
  batch 50 loss: 0.2738373079895973
  batch 100 loss: 0.2830414581298828
  batch 150 loss: 0.25541578337550164
  batch 200 loss: 0.2715082657337189
  batch 250 loss: 0.29502169013023377
  batch 300 loss: 0.3314200170338154
  batch 350 loss: 0.38820681393146517
  batch 400 loss: 0.3278425961732864
  batch 450 loss: 0.39322542905807495
  batch 500 loss: 0.32848996520042417
  batch 550 loss: 0.3055779954791069
  batch 600 loss: 0.29702351957559586
  batch 650 loss: 0.29049707695841787
  batch 700 loss: 0.30790687933564187
  batch 750 loss: 0.2720542347431183
  batch 800 loss: 0.27348988354206083
  batch 850 loss: 0.2909486252069473
  batch 900 loss: 0.29674877792596815
LOSS train 0.29675 valid 0.41958, valid PER 11.83%
EPOCH 8:
isNotFrozen  True
  batch 50 loss: 0.26818337142467497
  batch 100 loss: 0.3097241070866585
  batch 150 loss: 0.26992588490247726
  batch 200 loss: 0.2546632914245129
  batch 250 loss: 0.2541538986563683
  batch 300 loss: 0.2731496286392212
  batch 350 loss: 0.27692384392023084
  batch 400 loss: 0.2755464614927769
  batch 450 loss: 0.2666525655984879
  batch 500 loss: 0.25350722402334214
  batch 550 loss: 0.22490340247750282
  batch 600 loss: 0.24669808834791185
  batch 650 loss: 0.2548732335865498
  batch 700 loss: 0.25847224742174146
  batch 750 loss: 0.2500080554187298
  batch 800 loss: 0.24899068892002105
  batch 850 loss: 0.24697456173598767
  batch 900 loss: 0.2975398996472359
LOSS train 0.29754 valid 0.33402, valid PER 9.49%
EPOCH 9:
isNotFrozen  True
  batch 50 loss: 0.25165324434638026
  batch 100 loss: 0.24275964125990868
  batch 150 loss: 0.2446221876144409
  batch 200 loss: 0.2425331263244152
  batch 250 loss: 0.2396148882806301
  batch 300 loss: 0.22383179590106012
  batch 350 loss: 0.22426679760217666
  batch 400 loss: 0.4128331345319748
  batch 450 loss: 0.295395490527153
  batch 500 loss: 0.2536210943758488
  batch 550 loss: 0.4260190102458
  batch 600 loss: 0.3000945433974266
  batch 650 loss: 0.273716587126255
  batch 700 loss: 0.2688682858645916
  batch 750 loss: 0.29635921865701675
  batch 800 loss: 0.2960537686944008
  batch 850 loss: 0.2757094594836235
  batch 900 loss: 0.25743367686867713
LOSS train 0.25743 valid 0.34040, valid PER 9.71%
EPOCH 10:
isNotFrozen  True
  batch 50 loss: 0.197093897908926
  batch 100 loss: 0.21814374044537543
  batch 150 loss: 0.23840123951435088
  batch 200 loss: 0.24763704851269722
  batch 250 loss: 0.2393830254673958
  batch 300 loss: 0.2197297142446041
  batch 350 loss: 0.20562665671110153
  batch 400 loss: 0.217175312936306
  batch 450 loss: 0.22542958453297615
  batch 500 loss: 0.2766365134716034
  batch 550 loss: 0.2862262699007988
  batch 600 loss: 0.22427626043558122
  batch 650 loss: 0.231620661765337
  batch 700 loss: 0.2387827853858471
  batch 750 loss: 0.3288793185353279
  batch 800 loss: 0.32005456000566485
  batch 850 loss: 0.36518254786729815
  batch 900 loss: 0.2637589430809021
LOSS train 0.26376 valid 0.40801, valid PER 10.60%
EPOCH 11:
isNotFrozen  True
  batch 50 loss: 0.24868809819221496
  batch 100 loss: 0.23397719353437424
  batch 150 loss: 0.2129293304681778
  batch 200 loss: 0.21753489196300507
  batch 250 loss: 0.22649365037679672
  batch 300 loss: 0.2662436716258526
  batch 350 loss: 0.23024700805544854
  batch 400 loss: 0.4418119013309479
  batch 450 loss: 0.28754672050476077
  batch 500 loss: 0.2665092116594315
  batch 550 loss: 0.27383496060967444
  batch 600 loss: 0.2766495722532272
  batch 650 loss: 0.25087495386600495
  batch 700 loss: 0.25930248260498046
  batch 750 loss: 0.26077542811632154
  batch 800 loss: 0.2637687900662422
  batch 850 loss: 0.2674826470017433
  batch 900 loss: 0.25911167785525324
LOSS train 0.25911 valid 0.33869, valid PER 9.40%
EPOCH 12:
isNotFrozen  True
  batch 50 loss: 0.1940980665385723
  batch 100 loss: 0.19014096550643445
  batch 150 loss: 0.19056255653500556
  batch 200 loss: 0.21027391068637372
  batch 250 loss: 0.19855596721172333
  batch 300 loss: 0.20975547388195992
  batch 350 loss: 0.2249316756427288
  batch 400 loss: 0.21218563258647918
  batch 450 loss: 0.21407557286322118
  batch 500 loss: 0.21151026278734208
  batch 550 loss: 0.21108267053961755
  batch 600 loss: 0.21783479526638985
  batch 650 loss: 0.21875245332717896
  batch 700 loss: 0.22831592708826065
  batch 750 loss: 0.22705000951886178
  batch 800 loss: 0.2186742091178894
  batch 850 loss: 0.2283816710114479
  batch 900 loss: 0.23829873092472553
LOSS train 0.23830 valid 0.35244, valid PER 9.50%
EPOCH 13:
isNotFrozen  True
  batch 50 loss: 0.17265318632125853
  batch 100 loss: 0.1811184360086918
  batch 150 loss: 0.17727938525378703
  batch 200 loss: 0.21213425144553186
  batch 250 loss: 0.1793139946460724
  batch 300 loss: 0.21613258942961694
  batch 350 loss: 0.2188118116557598
  batch 400 loss: 0.23719453126192092
  batch 450 loss: 0.19861770316958427
  batch 500 loss: 0.2370510885119438
  batch 550 loss: 0.22731048345565796
  batch 600 loss: 0.21743969962000848
  batch 650 loss: 0.21799541845917703
  batch 700 loss: 0.20738769605755805
  batch 750 loss: 0.198768138512969
  batch 800 loss: 0.2155504544079304
  batch 850 loss: 0.22544800087809563
  batch 900 loss: 0.21251219764351845
LOSS train 0.21251 valid 0.32939, valid PER 8.86%
EPOCH 14:
isNotFrozen  True
  batch 50 loss: 0.15293131843209268
  batch 100 loss: 0.1797829756885767
  batch 150 loss: 0.17888994477689266
  batch 200 loss: 0.1982982251793146
  batch 250 loss: 0.19772575967013836
  batch 300 loss: 0.1719692063331604
  batch 350 loss: 0.1730451413989067
  batch 400 loss: 0.20336959764361381
  batch 450 loss: 0.19266744419932366
  batch 500 loss: 0.1803198854625225
  batch 550 loss: 0.20404876440763473
  batch 600 loss: 0.17883808493614198
  batch 650 loss: 0.1860221763700247
  batch 700 loss: 0.20661505609750747
  batch 750 loss: 0.20238437354564667
  batch 800 loss: 0.1738487909734249
  batch 850 loss: 0.19889081448316573
  batch 900 loss: 0.1973292987048626
LOSS train 0.19733 valid 0.36922, valid PER 9.26%
EPOCH 15:
isNotFrozen  True
  batch 50 loss: 0.17363216273486615
  batch 100 loss: 0.17224109724164008
  batch 150 loss: 0.1814647014439106
  batch 200 loss: 0.17073217809200286
  batch 250 loss: 0.19161606922745705
  batch 300 loss: 0.17926087141036987
  batch 350 loss: 0.1773524259030819
  batch 400 loss: 0.1829100951552391
  batch 450 loss: 0.18325775101780892
  batch 500 loss: 0.19185611382126808
  batch 550 loss: 0.1750188371539116
  batch 600 loss: 0.19906903430819511
  batch 650 loss: 0.19580844096839428
  batch 700 loss: 0.18066986188292503
  batch 750 loss: 0.1778472189605236
  batch 800 loss: 0.18394311510026454
  batch 850 loss: 0.18424437530338764
  batch 900 loss: 0.1744797808676958
LOSS train 0.17448 valid 0.36099, valid PER 9.67%
EPOCH 16:
isNotFrozen  True
  batch 50 loss: 0.1572441715747118
  batch 100 loss: 0.17595062762498856
  batch 150 loss: 0.17864735543727875
  batch 200 loss: 0.16224593237042428
  batch 250 loss: 0.18478019416332245
  batch 300 loss: 0.16910369396209718
  batch 350 loss: 0.192258363366127
  batch 400 loss: 0.19442134201526642
  batch 450 loss: 0.16942043669521809
  batch 500 loss: 0.1843932031095028
  batch 550 loss: 0.16684723876416682
  batch 600 loss: 0.18660413049161434
  batch 650 loss: 0.20169670954346658
  batch 700 loss: 0.17967735573649407
  batch 750 loss: 0.18001388400793075
  batch 800 loss: 0.21629440903663635
  batch 850 loss: 0.20964709252119065
  batch 900 loss: 0.18370628833770752
LOSS train 0.18371 valid 0.36608, valid PER 9.62%
EPOCH 17:
isNotFrozen  True
  batch 50 loss: 0.16190211415290834
  batch 100 loss: 0.18112211130559444
  batch 150 loss: 0.17181796059012414
  batch 200 loss: 0.17896564021706582
  batch 250 loss: 0.16393086060881615
  batch 300 loss: 0.1893284734338522
  batch 350 loss: 0.173474732786417
  batch 400 loss: 0.17287657514214516
  batch 450 loss: 0.185730701982975
  batch 500 loss: 0.24883956894278525
  batch 550 loss: 0.21150993257761003
  batch 600 loss: 0.21575278535485268
  batch 650 loss: 0.2025034323334694
  batch 700 loss: 0.20706098660826683
  batch 750 loss: 0.19619930043816566
  batch 800 loss: 0.17494475692510605
  batch 850 loss: 0.19081108003854752
  batch 900 loss: 0.15447904236614704
LOSS train 0.15448 valid 0.44011, valid PER 9.93%
EPOCH 18:
isNotFrozen  True
  batch 50 loss: 0.1948511128872633
  batch 100 loss: 0.1799285553395748
  batch 150 loss: 0.1544040247052908
  batch 200 loss: 0.16117942690849305
  batch 250 loss: 0.17668808802962302
  batch 300 loss: 0.17945604279637337
  batch 350 loss: 0.18475448369979858
  batch 400 loss: 0.1835626422613859
  batch 450 loss: 0.2033620961010456
  batch 500 loss: 0.18920331299304963
  batch 550 loss: 0.17497026950120925
  batch 600 loss: 0.19250257596373557
  batch 650 loss: 0.17725840479135513
  batch 700 loss: 0.21071962088346483
  batch 750 loss: 0.20297729790210725
  batch 800 loss: 0.1999174416065216
  batch 850 loss: 0.18810885950922965
  batch 900 loss: 0.232859937697649
LOSS train 0.23286 valid 0.41301, valid PER 9.83%
EPOCH 19:
isNotFrozen  True
  batch 50 loss: 0.1865094442665577
  batch 100 loss: 0.16272043138742448
  batch 150 loss: 0.1614943202584982
  batch 200 loss: 0.16077224217355252
  batch 250 loss: 0.15531451262533666
  batch 300 loss: 0.16048641685396434
  batch 350 loss: 0.18121113009750844
  batch 400 loss: 0.16164984211325645
  batch 450 loss: 0.17023538649082184
  batch 500 loss: 0.18750387214124203
  batch 550 loss: 0.1489339464902878
  batch 600 loss: 0.18097607672214508
  batch 650 loss: 0.18093493953347206
  batch 700 loss: 0.17916135258972646
  batch 750 loss: 0.18535585075616837
  batch 800 loss: 0.18111283391714095
  batch 850 loss: 0.14900915183126925
  batch 900 loss: 0.19203950688242913
LOSS train 0.19204 valid 0.41016, valid PER 9.30%
EPOCH 20:
isNotFrozen  True
  batch 50 loss: 0.14173922739923
  batch 100 loss: 0.13000919010490178
  batch 150 loss: 0.16133684173226356
  batch 200 loss: 0.14475760832428933
  batch 250 loss: 0.16044806137681009
  batch 300 loss: 0.1472340314090252
  batch 350 loss: 0.15978731490671635
  batch 400 loss: 0.16827787667512895
  batch 450 loss: 0.1478921227157116
  batch 500 loss: 0.15400206856429577
  batch 550 loss: 0.16831195279955863
  batch 600 loss: 0.16239064119756222
  batch 650 loss: 0.16288705624639988
  batch 700 loss: 0.16155787907540797
  batch 750 loss: 0.1634250721335411
  batch 800 loss: 0.18337567031383514
  batch 850 loss: 0.16663058541715145
  batch 900 loss: 0.18412547431886195
LOSS train 0.18413 valid 0.42959, valid PER 9.89%
Training finished in 27.0 minutes.
Model saved to checkpoints/20240131_133627/model_4
Loading model from checkpoints/20240131_133627/model_4
SUB: 5.80%, DEL: 2.63%, INS: 1.96%, COR: 91.58%, PER: 10.39%
