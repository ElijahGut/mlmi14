Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='adam', schedule_lr=True, freeze_layers=-1, inter_rep=0, combine_reps=False, warmup=False)
FROZEN LAYERS: 
masked_spec_embed False
feature_extractor.conv_layers.0.conv.weight False
feature_extractor.conv_layers.0.layer_norm.weight False
feature_extractor.conv_layers.0.layer_norm.bias False
feature_extractor.conv_layers.1.conv.weight False
feature_extractor.conv_layers.2.conv.weight False
feature_extractor.conv_layers.3.conv.weight False
feature_extractor.conv_layers.4.conv.weight False
feature_extractor.conv_layers.5.conv.weight False
feature_extractor.conv_layers.6.conv.weight False
feature_projection.layer_norm.weight False
feature_projection.layer_norm.bias False
feature_projection.projection.weight False
feature_projection.projection.bias False
encoder.pos_conv_embed.conv.bias False
encoder.pos_conv_embed.conv.weight_g False
encoder.pos_conv_embed.conv.weight_v False
encoder.layer_norm.weight False
encoder.layer_norm.bias False
encoder.layers.0.attention.k_proj.weight False
encoder.layers.0.attention.k_proj.bias False
encoder.layers.0.attention.v_proj.weight False
encoder.layers.0.attention.v_proj.bias False
encoder.layers.0.attention.q_proj.weight False
encoder.layers.0.attention.q_proj.bias False
encoder.layers.0.attention.out_proj.weight False
encoder.layers.0.attention.out_proj.bias False
encoder.layers.0.layer_norm.weight False
encoder.layers.0.layer_norm.bias False
encoder.layers.0.feed_forward.intermediate_dense.weight False
encoder.layers.0.feed_forward.intermediate_dense.bias False
encoder.layers.0.feed_forward.output_dense.weight False
encoder.layers.0.feed_forward.output_dense.bias False
encoder.layers.0.final_layer_norm.weight False
encoder.layers.0.final_layer_norm.bias False
encoder.layers.1.attention.k_proj.weight False
encoder.layers.1.attention.k_proj.bias False
encoder.layers.1.attention.v_proj.weight False
encoder.layers.1.attention.v_proj.bias False
encoder.layers.1.attention.q_proj.weight False
encoder.layers.1.attention.q_proj.bias False
encoder.layers.1.attention.out_proj.weight False
encoder.layers.1.attention.out_proj.bias False
encoder.layers.1.layer_norm.weight False
encoder.layers.1.layer_norm.bias False
encoder.layers.1.feed_forward.intermediate_dense.weight False
encoder.layers.1.feed_forward.intermediate_dense.bias False
encoder.layers.1.feed_forward.output_dense.weight False
encoder.layers.1.feed_forward.output_dense.bias False
encoder.layers.1.final_layer_norm.weight False
encoder.layers.1.final_layer_norm.bias False
encoder.layers.2.attention.k_proj.weight False
encoder.layers.2.attention.k_proj.bias False
encoder.layers.2.attention.v_proj.weight False
encoder.layers.2.attention.v_proj.bias False
encoder.layers.2.attention.q_proj.weight False
encoder.layers.2.attention.q_proj.bias False
encoder.layers.2.attention.out_proj.weight False
encoder.layers.2.attention.out_proj.bias False
encoder.layers.2.layer_norm.weight False
encoder.layers.2.layer_norm.bias False
encoder.layers.2.feed_forward.intermediate_dense.weight False
encoder.layers.2.feed_forward.intermediate_dense.bias False
encoder.layers.2.feed_forward.output_dense.weight False
encoder.layers.2.feed_forward.output_dense.bias False
encoder.layers.2.final_layer_norm.weight False
encoder.layers.2.final_layer_norm.bias False
encoder.layers.3.attention.k_proj.weight False
encoder.layers.3.attention.k_proj.bias False
encoder.layers.3.attention.v_proj.weight False
encoder.layers.3.attention.v_proj.bias False
encoder.layers.3.attention.q_proj.weight False
encoder.layers.3.attention.q_proj.bias False
encoder.layers.3.attention.out_proj.weight False
encoder.layers.3.attention.out_proj.bias False
encoder.layers.3.layer_norm.weight False
encoder.layers.3.layer_norm.bias False
encoder.layers.3.feed_forward.intermediate_dense.weight False
encoder.layers.3.feed_forward.intermediate_dense.bias False
encoder.layers.3.feed_forward.output_dense.weight False
encoder.layers.3.feed_forward.output_dense.bias False
encoder.layers.3.final_layer_norm.weight False
encoder.layers.3.final_layer_norm.bias False
encoder.layers.4.attention.k_proj.weight False
encoder.layers.4.attention.k_proj.bias False
encoder.layers.4.attention.v_proj.weight False
encoder.layers.4.attention.v_proj.bias False
encoder.layers.4.attention.q_proj.weight False
encoder.layers.4.attention.q_proj.bias False
encoder.layers.4.attention.out_proj.weight False
encoder.layers.4.attention.out_proj.bias False
encoder.layers.4.layer_norm.weight False
encoder.layers.4.layer_norm.bias False
encoder.layers.4.feed_forward.intermediate_dense.weight False
encoder.layers.4.feed_forward.intermediate_dense.bias False
encoder.layers.4.feed_forward.output_dense.weight False
encoder.layers.4.feed_forward.output_dense.bias False
encoder.layers.4.final_layer_norm.weight False
encoder.layers.4.final_layer_norm.bias False
encoder.layers.5.attention.k_proj.weight False
encoder.layers.5.attention.k_proj.bias False
encoder.layers.5.attention.v_proj.weight False
encoder.layers.5.attention.v_proj.bias False
encoder.layers.5.attention.q_proj.weight False
encoder.layers.5.attention.q_proj.bias False
encoder.layers.5.attention.out_proj.weight False
encoder.layers.5.attention.out_proj.bias False
encoder.layers.5.layer_norm.weight False
encoder.layers.5.layer_norm.bias False
encoder.layers.5.feed_forward.intermediate_dense.weight False
encoder.layers.5.feed_forward.intermediate_dense.bias False
encoder.layers.5.feed_forward.output_dense.weight False
encoder.layers.5.feed_forward.output_dense.bias False
encoder.layers.5.final_layer_norm.weight False
encoder.layers.5.final_layer_norm.bias False
encoder.layers.6.attention.k_proj.weight False
encoder.layers.6.attention.k_proj.bias False
encoder.layers.6.attention.v_proj.weight False
encoder.layers.6.attention.v_proj.bias False
encoder.layers.6.attention.q_proj.weight False
encoder.layers.6.attention.q_proj.bias False
encoder.layers.6.attention.out_proj.weight False
encoder.layers.6.attention.out_proj.bias False
encoder.layers.6.layer_norm.weight False
encoder.layers.6.layer_norm.bias False
encoder.layers.6.feed_forward.intermediate_dense.weight False
encoder.layers.6.feed_forward.intermediate_dense.bias False
encoder.layers.6.feed_forward.output_dense.weight False
encoder.layers.6.feed_forward.output_dense.bias False
encoder.layers.6.final_layer_norm.weight False
encoder.layers.6.final_layer_norm.bias False
encoder.layers.7.attention.k_proj.weight False
encoder.layers.7.attention.k_proj.bias False
encoder.layers.7.attention.v_proj.weight False
encoder.layers.7.attention.v_proj.bias False
encoder.layers.7.attention.q_proj.weight False
encoder.layers.7.attention.q_proj.bias False
encoder.layers.7.attention.out_proj.weight False
encoder.layers.7.attention.out_proj.bias False
encoder.layers.7.layer_norm.weight False
encoder.layers.7.layer_norm.bias False
encoder.layers.7.feed_forward.intermediate_dense.weight False
encoder.layers.7.feed_forward.intermediate_dense.bias False
encoder.layers.7.feed_forward.output_dense.weight False
encoder.layers.7.feed_forward.output_dense.bias False
encoder.layers.7.final_layer_norm.weight False
encoder.layers.7.final_layer_norm.bias False
encoder.layers.8.attention.k_proj.weight False
encoder.layers.8.attention.k_proj.bias False
encoder.layers.8.attention.v_proj.weight False
encoder.layers.8.attention.v_proj.bias False
encoder.layers.8.attention.q_proj.weight False
encoder.layers.8.attention.q_proj.bias False
encoder.layers.8.attention.out_proj.weight False
encoder.layers.8.attention.out_proj.bias False
encoder.layers.8.layer_norm.weight False
encoder.layers.8.layer_norm.bias False
encoder.layers.8.feed_forward.intermediate_dense.weight False
encoder.layers.8.feed_forward.intermediate_dense.bias False
encoder.layers.8.feed_forward.output_dense.weight False
encoder.layers.8.feed_forward.output_dense.bias False
encoder.layers.8.final_layer_norm.weight False
encoder.layers.8.final_layer_norm.bias False
encoder.layers.9.attention.k_proj.weight False
encoder.layers.9.attention.k_proj.bias False
encoder.layers.9.attention.v_proj.weight False
encoder.layers.9.attention.v_proj.bias False
encoder.layers.9.attention.q_proj.weight False
encoder.layers.9.attention.q_proj.bias False
encoder.layers.9.attention.out_proj.weight False
encoder.layers.9.attention.out_proj.bias False
encoder.layers.9.layer_norm.weight False
encoder.layers.9.layer_norm.bias False
encoder.layers.9.feed_forward.intermediate_dense.weight False
encoder.layers.9.feed_forward.intermediate_dense.bias False
encoder.layers.9.feed_forward.output_dense.weight False
encoder.layers.9.feed_forward.output_dense.bias False
encoder.layers.9.final_layer_norm.weight False
encoder.layers.9.final_layer_norm.bias False
encoder.layers.10.attention.k_proj.weight False
encoder.layers.10.attention.k_proj.bias False
encoder.layers.10.attention.v_proj.weight False
encoder.layers.10.attention.v_proj.bias False
encoder.layers.10.attention.q_proj.weight False
encoder.layers.10.attention.q_proj.bias False
encoder.layers.10.attention.out_proj.weight False
encoder.layers.10.attention.out_proj.bias False
encoder.layers.10.layer_norm.weight False
encoder.layers.10.layer_norm.bias False
encoder.layers.10.feed_forward.intermediate_dense.weight False
encoder.layers.10.feed_forward.intermediate_dense.bias False
encoder.layers.10.feed_forward.output_dense.weight False
encoder.layers.10.feed_forward.output_dense.bias False
encoder.layers.10.final_layer_norm.weight False
encoder.layers.10.final_layer_norm.bias False
encoder.layers.11.attention.k_proj.weight False
encoder.layers.11.attention.k_proj.bias False
encoder.layers.11.attention.v_proj.weight False
encoder.layers.11.attention.v_proj.bias False
encoder.layers.11.attention.q_proj.weight False
encoder.layers.11.attention.q_proj.bias False
encoder.layers.11.attention.out_proj.weight False
encoder.layers.11.attention.out_proj.bias False
encoder.layers.11.layer_norm.weight False
encoder.layers.11.layer_norm.bias False
encoder.layers.11.feed_forward.intermediate_dense.weight False
encoder.layers.11.feed_forward.intermediate_dense.bias False
encoder.layers.11.feed_forward.output_dense.weight False
encoder.layers.11.feed_forward.output_dense.bias False
encoder.layers.11.final_layer_norm.weight False
encoder.layers.11.final_layer_norm.bias False
Total number of model parameters is 94402472
TRAIN LOADER LENGTH/NUMBER OF STEPS,  924
total_steps 9240, first_milestone 4620.0, second_milestone 4620.0
EPOCH 1:
isNotFrozen  False
  batch 50 loss: 10.844059715270996
  batch 100 loss: 8.400217123031616
  batch 150 loss: 6.385603570938111
  batch 200 loss: 4.898172349929809
  batch 250 loss: 4.134865484237671
  batch 300 loss: 3.7762688636779784
  batch 350 loss: 3.5976204919815062
  batch 400 loss: 3.543195629119873
  batch 450 loss: 3.4370120000839233
  batch 500 loss: 3.390151515007019
  batch 550 loss: 3.3663063430786133
  batch 600 loss: 3.312824878692627
  batch 650 loss: 3.317909822463989
  batch 700 loss: 3.3315787220001223
  batch 750 loss: 3.2877059745788575
  batch 800 loss: 3.3010475778579713
  batch 850 loss: 3.288813772201538
  batch 900 loss: 3.261588621139526
last epoch eta,  [0.0001]
step count :924, len etas: 924
LOSS train 3.26159 valid 3.45876, valid PER 100.00%
EPOCH 2:
isNotFrozen  False
  batch 50 loss: 3.246632490158081
  batch 100 loss: 3.233462119102478
  batch 150 loss: 3.2413961935043334
  batch 200 loss: 3.217809977531433
  batch 250 loss: 3.2156884622573854
  batch 300 loss: 3.212425026893616
  batch 350 loss: 3.2064576959609985
  batch 400 loss: 3.2092755651474
  batch 450 loss: 3.18103542804718
  batch 500 loss: 3.202063775062561
  batch 550 loss: 3.180397391319275
  batch 600 loss: 3.16773118019104
  batch 650 loss: 3.158178367614746
  batch 700 loss: 3.156625819206238
  batch 750 loss: 3.1659404611587525
  batch 800 loss: 3.122802987098694
  batch 850 loss: 3.1315036010742188
  batch 900 loss: 3.1126901292800904
last epoch eta,  [0.0001]
step count :1848, len etas: 1848
LOSS train 3.11269 valid 3.21835, valid PER 98.30%
EPOCH 3:
isNotFrozen  False
  batch 50 loss: 3.1091738891601564
  batch 100 loss: 3.0788397693634035
  batch 150 loss: 3.0647918224334716
  batch 200 loss: 3.0530096387863157
  batch 250 loss: 3.015265645980835
  batch 300 loss: 3.03112717628479
  batch 350 loss: 3.025545792579651
  batch 400 loss: 2.9929746103286745
  batch 450 loss: 3.0049933910369875
  batch 500 loss: 2.96702552318573
  batch 550 loss: 2.943100781440735
  batch 600 loss: 2.93943470954895
  batch 650 loss: 2.873473219871521
  batch 700 loss: 2.8625167560577394
  batch 750 loss: 2.8997726535797117
  batch 800 loss: 2.8470313262939455
  batch 850 loss: 2.8404242324829103
  batch 900 loss: 2.803908696174622
last epoch eta,  [0.0001]
step count :2772, len etas: 2772
LOSS train 2.80391 valid 2.82638, valid PER 83.06%
UNFROZEN LAYERS: 
masked_spec_embed True
feature_extractor.conv_layers.0.conv.weight False
feature_extractor.conv_layers.0.layer_norm.weight False
feature_extractor.conv_layers.0.layer_norm.bias False
feature_extractor.conv_layers.1.conv.weight False
feature_extractor.conv_layers.2.conv.weight False
feature_extractor.conv_layers.3.conv.weight False
feature_extractor.conv_layers.4.conv.weight False
feature_extractor.conv_layers.5.conv.weight False
feature_extractor.conv_layers.6.conv.weight False
feature_projection.layer_norm.weight True
feature_projection.layer_norm.bias True
feature_projection.projection.weight True
feature_projection.projection.bias True
encoder.pos_conv_embed.conv.bias True
encoder.pos_conv_embed.conv.weight_g True
encoder.pos_conv_embed.conv.weight_v True
encoder.layer_norm.weight True
encoder.layer_norm.bias True
encoder.layers.0.attention.k_proj.weight True
encoder.layers.0.attention.k_proj.bias True
encoder.layers.0.attention.v_proj.weight True
encoder.layers.0.attention.v_proj.bias True
encoder.layers.0.attention.q_proj.weight True
encoder.layers.0.attention.q_proj.bias True
encoder.layers.0.attention.out_proj.weight True
encoder.layers.0.attention.out_proj.bias True
encoder.layers.0.layer_norm.weight True
encoder.layers.0.layer_norm.bias True
encoder.layers.0.feed_forward.intermediate_dense.weight True
encoder.layers.0.feed_forward.intermediate_dense.bias True
encoder.layers.0.feed_forward.output_dense.weight True
encoder.layers.0.feed_forward.output_dense.bias True
encoder.layers.0.final_layer_norm.weight True
encoder.layers.0.final_layer_norm.bias True
encoder.layers.1.attention.k_proj.weight True
encoder.layers.1.attention.k_proj.bias True
encoder.layers.1.attention.v_proj.weight True
encoder.layers.1.attention.v_proj.bias True
encoder.layers.1.attention.q_proj.weight True
encoder.layers.1.attention.q_proj.bias True
encoder.layers.1.attention.out_proj.weight True
encoder.layers.1.attention.out_proj.bias True
encoder.layers.1.layer_norm.weight True
encoder.layers.1.layer_norm.bias True
encoder.layers.1.feed_forward.intermediate_dense.weight True
encoder.layers.1.feed_forward.intermediate_dense.bias True
encoder.layers.1.feed_forward.output_dense.weight True
encoder.layers.1.feed_forward.output_dense.bias True
encoder.layers.1.final_layer_norm.weight True
encoder.layers.1.final_layer_norm.bias True
encoder.layers.2.attention.k_proj.weight True
encoder.layers.2.attention.k_proj.bias True
encoder.layers.2.attention.v_proj.weight True
encoder.layers.2.attention.v_proj.bias True
encoder.layers.2.attention.q_proj.weight True
encoder.layers.2.attention.q_proj.bias True
encoder.layers.2.attention.out_proj.weight True
encoder.layers.2.attention.out_proj.bias True
encoder.layers.2.layer_norm.weight True
encoder.layers.2.layer_norm.bias True
encoder.layers.2.feed_forward.intermediate_dense.weight True
encoder.layers.2.feed_forward.intermediate_dense.bias True
encoder.layers.2.feed_forward.output_dense.weight True
encoder.layers.2.feed_forward.output_dense.bias True
encoder.layers.2.final_layer_norm.weight True
encoder.layers.2.final_layer_norm.bias True
encoder.layers.3.attention.k_proj.weight True
encoder.layers.3.attention.k_proj.bias True
encoder.layers.3.attention.v_proj.weight True
encoder.layers.3.attention.v_proj.bias True
encoder.layers.3.attention.q_proj.weight True
encoder.layers.3.attention.q_proj.bias True
encoder.layers.3.attention.out_proj.weight True
encoder.layers.3.attention.out_proj.bias True
encoder.layers.3.layer_norm.weight True
encoder.layers.3.layer_norm.bias True
encoder.layers.3.feed_forward.intermediate_dense.weight True
encoder.layers.3.feed_forward.intermediate_dense.bias True
encoder.layers.3.feed_forward.output_dense.weight True
encoder.layers.3.feed_forward.output_dense.bias True
encoder.layers.3.final_layer_norm.weight True
encoder.layers.3.final_layer_norm.bias True
encoder.layers.4.attention.k_proj.weight True
encoder.layers.4.attention.k_proj.bias True
encoder.layers.4.attention.v_proj.weight True
encoder.layers.4.attention.v_proj.bias True
encoder.layers.4.attention.q_proj.weight True
encoder.layers.4.attention.q_proj.bias True
encoder.layers.4.attention.out_proj.weight True
encoder.layers.4.attention.out_proj.bias True
encoder.layers.4.layer_norm.weight True
encoder.layers.4.layer_norm.bias True
encoder.layers.4.feed_forward.intermediate_dense.weight True
encoder.layers.4.feed_forward.intermediate_dense.bias True
encoder.layers.4.feed_forward.output_dense.weight True
encoder.layers.4.feed_forward.output_dense.bias True
encoder.layers.4.final_layer_norm.weight True
encoder.layers.4.final_layer_norm.bias True
encoder.layers.5.attention.k_proj.weight True
encoder.layers.5.attention.k_proj.bias True
encoder.layers.5.attention.v_proj.weight True
encoder.layers.5.attention.v_proj.bias True
encoder.layers.5.attention.q_proj.weight True
encoder.layers.5.attention.q_proj.bias True
encoder.layers.5.attention.out_proj.weight True
encoder.layers.5.attention.out_proj.bias True
encoder.layers.5.layer_norm.weight True
encoder.layers.5.layer_norm.bias True
encoder.layers.5.feed_forward.intermediate_dense.weight True
encoder.layers.5.feed_forward.intermediate_dense.bias True
encoder.layers.5.feed_forward.output_dense.weight True
encoder.layers.5.feed_forward.output_dense.bias True
encoder.layers.5.final_layer_norm.weight True
encoder.layers.5.final_layer_norm.bias True
encoder.layers.6.attention.k_proj.weight True
encoder.layers.6.attention.k_proj.bias True
encoder.layers.6.attention.v_proj.weight True
encoder.layers.6.attention.v_proj.bias True
encoder.layers.6.attention.q_proj.weight True
encoder.layers.6.attention.q_proj.bias True
encoder.layers.6.attention.out_proj.weight True
encoder.layers.6.attention.out_proj.bias True
encoder.layers.6.layer_norm.weight True
encoder.layers.6.layer_norm.bias True
encoder.layers.6.feed_forward.intermediate_dense.weight True
encoder.layers.6.feed_forward.intermediate_dense.bias True
encoder.layers.6.feed_forward.output_dense.weight True
encoder.layers.6.feed_forward.output_dense.bias True
encoder.layers.6.final_layer_norm.weight True
encoder.layers.6.final_layer_norm.bias True
encoder.layers.7.attention.k_proj.weight True
encoder.layers.7.attention.k_proj.bias True
encoder.layers.7.attention.v_proj.weight True
encoder.layers.7.attention.v_proj.bias True
encoder.layers.7.attention.q_proj.weight True
encoder.layers.7.attention.q_proj.bias True
encoder.layers.7.attention.out_proj.weight True
encoder.layers.7.attention.out_proj.bias True
encoder.layers.7.layer_norm.weight True
encoder.layers.7.layer_norm.bias True
encoder.layers.7.feed_forward.intermediate_dense.weight True
encoder.layers.7.feed_forward.intermediate_dense.bias True
encoder.layers.7.feed_forward.output_dense.weight True
encoder.layers.7.feed_forward.output_dense.bias True
encoder.layers.7.final_layer_norm.weight True
encoder.layers.7.final_layer_norm.bias True
encoder.layers.8.attention.k_proj.weight True
encoder.layers.8.attention.k_proj.bias True
encoder.layers.8.attention.v_proj.weight True
encoder.layers.8.attention.v_proj.bias True
encoder.layers.8.attention.q_proj.weight True
encoder.layers.8.attention.q_proj.bias True
encoder.layers.8.attention.out_proj.weight True
encoder.layers.8.attention.out_proj.bias True
encoder.layers.8.layer_norm.weight True
encoder.layers.8.layer_norm.bias True
encoder.layers.8.feed_forward.intermediate_dense.weight True
encoder.layers.8.feed_forward.intermediate_dense.bias True
encoder.layers.8.feed_forward.output_dense.weight True
encoder.layers.8.feed_forward.output_dense.bias True
encoder.layers.8.final_layer_norm.weight True
encoder.layers.8.final_layer_norm.bias True
encoder.layers.9.attention.k_proj.weight True
encoder.layers.9.attention.k_proj.bias True
encoder.layers.9.attention.v_proj.weight True
encoder.layers.9.attention.v_proj.bias True
encoder.layers.9.attention.q_proj.weight True
encoder.layers.9.attention.q_proj.bias True
encoder.layers.9.attention.out_proj.weight True
encoder.layers.9.attention.out_proj.bias True
encoder.layers.9.layer_norm.weight True
encoder.layers.9.layer_norm.bias True
encoder.layers.9.feed_forward.intermediate_dense.weight True
encoder.layers.9.feed_forward.intermediate_dense.bias True
encoder.layers.9.feed_forward.output_dense.weight True
encoder.layers.9.feed_forward.output_dense.bias True
encoder.layers.9.final_layer_norm.weight True
encoder.layers.9.final_layer_norm.bias True
encoder.layers.10.attention.k_proj.weight True
encoder.layers.10.attention.k_proj.bias True
encoder.layers.10.attention.v_proj.weight True
encoder.layers.10.attention.v_proj.bias True
encoder.layers.10.attention.q_proj.weight True
encoder.layers.10.attention.q_proj.bias True
encoder.layers.10.attention.out_proj.weight True
encoder.layers.10.attention.out_proj.bias True
encoder.layers.10.layer_norm.weight True
encoder.layers.10.layer_norm.bias True
encoder.layers.10.feed_forward.intermediate_dense.weight True
encoder.layers.10.feed_forward.intermediate_dense.bias True
encoder.layers.10.feed_forward.output_dense.weight True
encoder.layers.10.feed_forward.output_dense.bias True
encoder.layers.10.final_layer_norm.weight True
encoder.layers.10.final_layer_norm.bias True
encoder.layers.11.attention.k_proj.weight True
encoder.layers.11.attention.k_proj.bias True
encoder.layers.11.attention.v_proj.weight True
encoder.layers.11.attention.v_proj.bias True
encoder.layers.11.attention.q_proj.weight True
encoder.layers.11.attention.q_proj.bias True
encoder.layers.11.attention.out_proj.weight True
encoder.layers.11.attention.out_proj.bias True
encoder.layers.11.layer_norm.weight True
encoder.layers.11.layer_norm.bias True
encoder.layers.11.feed_forward.intermediate_dense.weight True
encoder.layers.11.feed_forward.intermediate_dense.bias True
encoder.layers.11.feed_forward.output_dense.weight True
encoder.layers.11.feed_forward.output_dense.bias True
encoder.layers.11.final_layer_norm.weight True
encoder.layers.11.final_layer_norm.bias True
EPOCH 4:
isNotFrozen  True
  batch 50 loss: 1.9431222009658813
  batch 100 loss: 1.1071231496334075
  batch 150 loss: 0.739466336965561
  batch 200 loss: 0.6848565131425858
  batch 250 loss: 0.5779332590103149
  batch 300 loss: 0.5902833014726638
  batch 350 loss: 0.5537543094158173
  batch 400 loss: 0.5453229528665543
  batch 450 loss: 0.520532392859459
  batch 500 loss: 0.4834491693973541
  batch 550 loss: 0.4921549952030182
  batch 600 loss: 0.5050229912996292
  batch 650 loss: 0.4515143305063248
  batch 700 loss: 0.4801433289051056
  batch 750 loss: 0.4412319791316986
  batch 800 loss: 0.4339910650253296
  batch 850 loss: 0.46193975567817686
  batch 900 loss: 0.4786674052476883
last epoch eta,  [0.0001]
step count :3696, len etas: 3696
LOSS train 0.47867 valid 0.32508, valid PER 9.61%
EPOCH 5:
isNotFrozen  True
  batch 50 loss: 0.38137882947921753
  batch 100 loss: 0.3943886497616768
  batch 150 loss: 0.41265096783638
  batch 200 loss: 0.4172990369796753
  batch 250 loss: 0.39773606926202776
  batch 300 loss: 0.3935586440563202
  batch 350 loss: 0.36612894117832184
  batch 400 loss: 0.3835915768146515
  batch 450 loss: 0.35078756719827653
  batch 500 loss: 0.38817011505365373
  batch 550 loss: 0.3647574132680893
  batch 600 loss: 0.3642181742191315
  batch 650 loss: 0.38473360061645506
  batch 700 loss: 0.42942503452301023
  batch 750 loss: 0.3655747899413109
  batch 800 loss: 0.3637378144264221
  batch 850 loss: 0.34614388525485995
  batch 900 loss: 0.36565675646066664
last epoch eta,  [0.0001]
step count :4620, len etas: 4620
LOSS train 0.36566 valid 0.28658, valid PER 8.73%
EPOCH 6:
isNotFrozen  True
  batch 50 loss: 0.30082196950912476
  batch 100 loss: 0.31088300436735156
  batch 150 loss: 0.30821105420589445
  batch 200 loss: 0.31250998109579087
  batch 250 loss: 0.3434641283750534
  batch 300 loss: 0.34007638722658157
  batch 350 loss: 0.30800280302762983
  batch 400 loss: 0.3176753003895283
  batch 450 loss: 0.32941859573125837
  batch 500 loss: 0.31868287563323977
  batch 550 loss: 0.29833909034729006
  batch 600 loss: 0.304444345831871
  batch 650 loss: 0.31940171599388123
  batch 700 loss: 0.3186290630698204
  batch 750 loss: 0.3356015285849571
  batch 800 loss: 0.35001610428094865
  batch 850 loss: 0.3041465073823929
  batch 900 loss: 0.3307126584649086
last epoch eta,  [7.999999999999974e-05]
step count :5544, len etas: 5544
LOSS train 0.33071 valid 0.27688, valid PER 8.33%
EPOCH 7:
isNotFrozen  True
  batch 50 loss: 0.26622276425361635
  batch 100 loss: 0.2889959445595741
  batch 150 loss: 0.269471547305584
  batch 200 loss: 0.2673141637444496
  batch 250 loss: 0.2627114447951317
  batch 300 loss: 0.2849209687113762
  batch 350 loss: 0.29972542002797126
  batch 400 loss: 0.27315078914165497
  batch 450 loss: 0.2895335653424263
  batch 500 loss: 0.27806474804878234
  batch 550 loss: 0.2554862180352211
  batch 600 loss: 0.26860491052269936
  batch 650 loss: 0.2785649171471596
  batch 700 loss: 0.2699668690562248
  batch 750 loss: 0.2531184005737305
  batch 800 loss: 0.28124499022960664
  batch 850 loss: 0.24625898838043214
  batch 900 loss: 0.2570944982767105
last epoch eta,  [6.0000000000000123e-05]
step count :6468, len etas: 6468
LOSS train 0.25709 valid 0.26379, valid PER 8.27%
EPOCH 8:
isNotFrozen  True
  batch 50 loss: 0.21843887850642205
  batch 100 loss: 0.2116248321533203
  batch 150 loss: 0.230069423019886
  batch 200 loss: 0.22662075445055962
  batch 250 loss: 0.22081377908587455
  batch 300 loss: 0.20233116135001183
  batch 350 loss: 0.22204915702342987
  batch 400 loss: 0.23022281497716904
  batch 450 loss: 0.22261587843298913
  batch 500 loss: 0.2162861467897892
  batch 550 loss: 0.20971744418144225
  batch 600 loss: 0.20654768660664558
  batch 650 loss: 0.20980796232819557
  batch 700 loss: 0.2291973465681076
  batch 750 loss: 0.2004515601694584
  batch 800 loss: 0.2372860586643219
  batch 850 loss: 0.21727676838636398
  batch 900 loss: 0.2332076521217823
last epoch eta,  [3.999999999999979e-05]
step count :7392, len etas: 7392
LOSS train 0.23321 valid 0.26468, valid PER 7.89%
EPOCH 9:
isNotFrozen  True
  batch 50 loss: 0.19693846374750137
  batch 100 loss: 0.19891238033771516
  batch 150 loss: 0.17735613226890565
  batch 200 loss: 0.1740335688740015
  batch 250 loss: 0.18967144399881364
  batch 300 loss: 0.18572880655527116
  batch 350 loss: 0.19144990131258965
  batch 400 loss: 0.1622323662042618
  batch 450 loss: 0.17759645834565163
  batch 500 loss: 0.1783050502836704
  batch 550 loss: 0.1963350959122181
  batch 600 loss: 0.1954440400749445
  batch 650 loss: 0.16957361355423928
  batch 700 loss: 0.18805372834205628
  batch 750 loss: 0.1812792181968689
  batch 800 loss: 0.17036395691335202
  batch 850 loss: 0.18472373574972153
  batch 900 loss: 0.17010257303714751
last epoch eta,  [1.9999999999999653e-05]
step count :8316, len etas: 8316
LOSS train 0.17010 valid 0.26519, valid PER 7.91%
EPOCH 10:
isNotFrozen  True
  batch 50 loss: 0.159379763007164
  batch 100 loss: 0.1576854580640793
  batch 150 loss: 0.15200780153274537
  batch 200 loss: 0.15424812242388725
  batch 250 loss: 0.17061953380703926
  batch 300 loss: 0.13571509666740894
  batch 350 loss: 0.157964748442173
  batch 400 loss: 0.16219689324498177
  batch 450 loss: 0.16116727400571107
  batch 500 loss: 0.15234411567449568
  batch 550 loss: 0.15593414269387723
  batch 600 loss: 0.16476947590708732
  batch 650 loss: 0.1506896597146988
  batch 700 loss: 0.1413619463890791
  batch 750 loss: 0.14743119545280933
  batch 800 loss: 0.15734547570347787
  batch 850 loss: 0.1545294179022312
  batch 900 loss: 0.14177848115563393
last epoch eta,  [0.0]
step count :9240, len etas: 9240
LOSS train 0.14178 valid 0.27067, valid PER 7.69%
Training finished in 10.0 minutes.
Model saved to checkpoints/20240210_202148/model_7
Loading model from checkpoints/20240210_202148/model_7
CLEAN
 SUB: 5.16%, DEL: 2.54%, INS: 1.71%, COR: 92.30%, PER: 9.41%

