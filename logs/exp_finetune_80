Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='adam', schedule_lr=True, freeze_layers=-1, first_milestone=0.8)
Total number of model parameters is 94402472
Adjusting learning rate of group 0 to 1.0000e-04.
Adjusting learning rate of group 0 to 1.0000e-04.
EPOCH 1:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 11.179888038635253
  batch 100 loss: 8.841108274459838
  batch 150 loss: 6.73566502571106
  batch 200 loss: 5.073895816802978
  batch 250 loss: 4.193756508827209
  batch 300 loss: 3.8150962257385252
  batch 350 loss: 3.6065159177780153
  batch 400 loss: 3.5230155992507934
  batch 450 loss: 3.4332162666320802
  batch 500 loss: 3.3889887857437135
  batch 550 loss: 3.381075391769409
  batch 600 loss: 3.3247391271591185
  batch 650 loss: 3.328166422843933
  batch 700 loss: 3.3405023574829102
  batch 750 loss: 3.30704779624939
  batch 800 loss: 3.2999666261672975
  batch 850 loss: 3.2987437868118286
  batch 900 loss: 3.276934928894043
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.27693 valid 3.46582, valid PER 99.97%
EPOCH 2:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.2525333261489866
  batch 100 loss: 3.2461440706253053
  batch 150 loss: 3.2474743127822876
  batch 200 loss: 3.2371953105926514
  batch 250 loss: 3.234054055213928
  batch 300 loss: 3.2342656517028807
  batch 350 loss: 3.2269539880752562
  batch 400 loss: 3.2220357513427733
  batch 450 loss: 3.2030720376968382
  batch 500 loss: 3.195561895370483
  batch 550 loss: 3.190491795539856
  batch 600 loss: 3.167690658569336
  batch 650 loss: 3.187437825202942
  batch 700 loss: 3.1624025440216066
  batch 750 loss: 3.169535994529724
  batch 800 loss: 3.1357336473464965
  batch 850 loss: 3.148465871810913
  batch 900 loss: 3.1471576356887816
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.14716 valid 3.22274, valid PER 98.05%
EPOCH 3:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.127699890136719
  batch 100 loss: 3.096346712112427
  batch 150 loss: 3.088861389160156
  batch 200 loss: 3.07186315536499
  batch 250 loss: 3.0558336210250854
  batch 300 loss: 3.0563214206695557
  batch 350 loss: 3.051925497055054
  batch 400 loss: 3.020289645195007
  batch 450 loss: 3.0398925876617433
  batch 500 loss: 2.9847812604904176
  batch 550 loss: 2.970889482498169
  batch 600 loss: 2.936580696105957
  batch 650 loss: 2.9021331787109377
  batch 700 loss: 2.905853886604309
  batch 750 loss: 2.9101743030548097
  batch 800 loss: 2.880910048484802
  batch 850 loss: 2.8570068645477296
  batch 900 loss: 2.8332754230499266
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 2.83328 valid 2.83459, valid PER 82.91%
EPOCH 4:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 1.9818591785430908
  batch 100 loss: 1.1401698005199432
  batch 150 loss: 0.7838240420818329
  batch 200 loss: 0.7146859675645828
  batch 250 loss: 0.6932690149545669
  batch 300 loss: 0.6254032897949219
  batch 350 loss: 0.5616675055027008
  batch 400 loss: 0.5588796406984329
  batch 450 loss: 0.5433987528085709
  batch 500 loss: 0.5427237665653228
  batch 550 loss: 0.5643652236461639
  batch 600 loss: 0.5516064536571502
  batch 650 loss: 0.4984014555811882
  batch 700 loss: 0.509477436542511
  batch 750 loss: 0.48351145565509795
  batch 800 loss: 0.5172570103406906
  batch 850 loss: 0.6660690951347351
  batch 900 loss: 0.6699378871917725
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.66994 valid 0.39190, valid PER 10.75%
EPOCH 5:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.5525733691453933
  batch 100 loss: 0.44549151360988615
  batch 150 loss: 0.4602942994236946
  batch 200 loss: 0.43972797214984893
  batch 250 loss: 0.45127312660217284
  batch 300 loss: 0.4272387272119522
  batch 350 loss: 0.43251865267753603
  batch 400 loss: 0.4495189821720123
  batch 450 loss: 0.4339313167333603
  batch 500 loss: 0.5693231523036957
  batch 550 loss: 0.4841814464330673
  batch 600 loss: 0.4663548183441162
  batch 650 loss: 0.438215537071228
  batch 700 loss: 0.48997793078422547
  batch 750 loss: 0.4218245929479599
  batch 800 loss: 0.4332129633426666
  batch 850 loss: 0.4257884466648102
  batch 900 loss: 0.4633919322490692
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.46339 valid 0.35783, valid PER 10.59%
EPOCH 6:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.4408446502685547
  batch 100 loss: 0.4613021731376648
  batch 150 loss: 0.42939061284065244
  batch 200 loss: 0.5576331382989883
  batch 250 loss: 0.45257417142391204
  batch 300 loss: 0.43089938670396805
  batch 350 loss: 0.42102442532777784
  batch 400 loss: 0.3903056737780571
  batch 450 loss: 0.3954364576935768
  batch 500 loss: 0.37745546251535417
  batch 550 loss: 0.36292417228221896
  batch 600 loss: 0.3702718153595924
  batch 650 loss: 0.37105132281780245
  batch 700 loss: 0.36991611510515215
  batch 750 loss: 0.37433682978153227
  batch 800 loss: 0.38437373042106626
  batch 850 loss: 0.3658249118924141
  batch 900 loss: 0.3954596894979477
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.39546 valid 0.31560, valid PER 9.23%
EPOCH 7:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.349930394589901
  batch 100 loss: 0.32976565808057784
  batch 150 loss: 0.2928155641257763
  batch 200 loss: 0.366494910120964
  batch 250 loss: 0.3260841715335846
  batch 300 loss: 0.3302310642600059
  batch 350 loss: 0.36101558089256286
  batch 400 loss: 0.33383071452379226
  batch 450 loss: 0.33251873284578326
  batch 500 loss: 0.36453108847141263
  batch 550 loss: 0.3303258588910103
  batch 600 loss: 0.33425693333148954
  batch 650 loss: 0.3681811386346817
  batch 700 loss: 0.3752823716402054
  batch 750 loss: 0.34508238852024076
  batch 800 loss: 0.3563689750432968
  batch 850 loss: 0.3399846249818802
  batch 900 loss: 0.3414238712191582
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.34142 valid 0.35904, valid PER 10.35%
EPOCH 8:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.3597689148783684
  batch 100 loss: 0.30828162491321565
  batch 150 loss: 0.30732775300741194
  batch 200 loss: 0.31584138065576556
  batch 250 loss: 0.3191297650337219
  batch 300 loss: 0.28299164444208147
  batch 350 loss: 0.3153562143445015
  batch 400 loss: 0.3314104378223419
  batch 450 loss: 0.3252777937054634
  batch 500 loss: 0.3018771062791348
  batch 550 loss: 0.30303472876548765
  batch 600 loss: 0.3392053806781769
  batch 650 loss: 0.30536621779203416
  batch 700 loss: 0.3152898561954498
  batch 750 loss: 0.3328972253203392
  batch 800 loss: 0.3519690892100334
  batch 850 loss: 0.3396073603630066
  batch 900 loss: 0.4361445614695549
Adjusting learning rate of group 0 to 6.6667e-05.
LOSS train 0.43614 valid 0.38416, valid PER 11.17%
EPOCH 9:
ETA [6.666666666666668e-05]

isNotFrozen  True
  batch 50 loss: 0.3062896099686623
  batch 100 loss: 0.31250257790088654
  batch 150 loss: 0.25762724831700323
  batch 200 loss: 0.24897111132740973
  batch 250 loss: 0.2848586314916611
  batch 300 loss: 0.27280679166316985
  batch 350 loss: 0.2627469649910927
  batch 400 loss: 0.26922302156686784
  batch 450 loss: 0.28822593063116075
  batch 500 loss: 0.30390255302190783
  batch 550 loss: 0.28127082020044325
  batch 600 loss: 0.276167109310627
  batch 650 loss: 0.2874836294353008
  batch 700 loss: 0.2736742900311947
  batch 750 loss: 0.24332281067967415
  batch 800 loss: 0.2718759359419346
  batch 850 loss: 0.2795496717095375
  batch 900 loss: 0.2579135848581791
Adjusting learning rate of group 0 to 3.3333e-05.
LOSS train 0.25791 valid 0.29230, valid PER 8.89%
EPOCH 10:
ETA [3.333333333333334e-05]

isNotFrozen  True
  batch 50 loss: 0.21077896758913994
  batch 100 loss: 0.23248000040650368
  batch 150 loss: 0.23189001217484473
  batch 200 loss: 0.21438745379447938
  batch 250 loss: 0.21089043602347374
  batch 300 loss: 0.21224520578980446
  batch 350 loss: 0.19846614003181456
  batch 400 loss: 0.20830506339669227
  batch 450 loss: 0.2210285721719265
  batch 500 loss: 0.20625077918171883
  batch 550 loss: 0.22922987267374992
  batch 600 loss: 0.20998113051056863
  batch 650 loss: 0.20540276139974595
  batch 700 loss: 0.20275939598679543
  batch 750 loss: 0.19781672075390816
  batch 800 loss: 0.21434236206114293
  batch 850 loss: 0.20672857165336608
  batch 900 loss: 0.18144152380526066
Adjusting learning rate of group 0 to 0.0000e+00.
LOSS train 0.18144 valid 0.28372, valid PER 8.41%
Training finished in 12.0 minutes.
Model saved to checkpoints/20240201_092714/model_10
Loading model from checkpoints/20240201_092714/model_10
SUB: 5.80%, DEL: 2.17%, INS: 2.11%, COR: 92.03%, PER: 10.08%
