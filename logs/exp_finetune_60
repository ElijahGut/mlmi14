Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='adam', schedule_lr=True, freeze_layers=-1)
Total number of model parameters is 94402472
Adjusting learning rate of group 0 to 1.0000e-04.
Adjusting learning rate of group 0 to 1.0000e-04.
EPOCH 1:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 11.110277194976806
  batch 100 loss: 8.76274868965149
  batch 150 loss: 6.818651666641236
  batch 200 loss: 5.207717313766479
  batch 250 loss: 4.295371675491333
  batch 300 loss: 3.829027543067932
  batch 350 loss: 3.608272686004639
  batch 400 loss: 3.5443339347839355
  batch 450 loss: 3.4343642663955687
  batch 500 loss: 3.4032955837249754
  batch 550 loss: 3.3888009881973264
  batch 600 loss: 3.319706802368164
  batch 650 loss: 3.3372316646575926
  batch 700 loss: 3.345808916091919
  batch 750 loss: 3.298594560623169
  batch 800 loss: 3.29683539390564
  batch 850 loss: 3.29467209815979
  batch 900 loss: 3.2741858673095705
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.27419 valid 3.48190, valid PER 100.00%
EPOCH 2:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.251792802810669
  batch 100 loss: 3.2489115476608275
  batch 150 loss: 3.257370476722717
  batch 200 loss: 3.2244534635543824
  batch 250 loss: 3.2421977663040162
  batch 300 loss: 3.2269790029525756
  batch 350 loss: 3.203117275238037
  batch 400 loss: 3.218870606422424
  batch 450 loss: 3.1844932556152346
  batch 500 loss: 3.199271755218506
  batch 550 loss: 3.1764583921432497
  batch 600 loss: 3.154277763366699
  batch 650 loss: 3.1631147480010986
  batch 700 loss: 3.1544087505340577
  batch 750 loss: 3.1528560590744017
  batch 800 loss: 3.129787173271179
  batch 850 loss: 3.1159674310684204
  batch 900 loss: 3.112263536453247
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.11226 valid 3.19837, valid PER 98.23%
EPOCH 3:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.085909662246704
  batch 100 loss: 3.0663517475128175
  batch 150 loss: 3.061590700149536
  batch 200 loss: 3.0506538105010987
  batch 250 loss: 3.0217936992645265
  batch 300 loss: 3.0314232158660888
  batch 350 loss: 3.0134778738021852
  batch 400 loss: 2.9904127883911134
  batch 450 loss: 2.9780171155929565
  batch 500 loss: 2.9408156442642213
  batch 550 loss: 2.9199409914016723
  batch 600 loss: 2.9151866149902346
  batch 650 loss: 2.8520677852630616
  batch 700 loss: 2.848988275527954
  batch 750 loss: 2.8587919664382935
  batch 800 loss: 2.8308254146575926
  batch 850 loss: 2.8224677658081054
  batch 900 loss: 2.774571876525879
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 2.77457 valid 2.81041, valid PER 82.69%
EPOCH 4:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 1.8202593898773194
  batch 100 loss: 1.0796319210529328
  batch 150 loss: 0.7363997530937195
  batch 200 loss: 0.7511879080533981
  batch 250 loss: 0.6542838525772094
  batch 300 loss: 0.6226939702033997
  batch 350 loss: 0.5803914296627045
  batch 400 loss: 0.6763651126623154
  batch 450 loss: 0.5801275759935379
  batch 500 loss: 0.567947992682457
  batch 550 loss: 0.6312917637825012
  batch 600 loss: 0.6196058040857315
  batch 650 loss: 0.5412549453973771
  batch 700 loss: 0.5278489291667938
  batch 750 loss: 0.5179015427827836
  batch 800 loss: 0.4897992190718651
  batch 850 loss: 0.49177255272865295
  batch 900 loss: 0.5165057334303856
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.51651 valid 0.36829, valid PER 10.65%
EPOCH 5:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.46097227692604065
  batch 100 loss: 0.42749287843704226
  batch 150 loss: 0.4928887742757797
  batch 200 loss: 0.4124780198931694
  batch 250 loss: 0.4352495664358139
  batch 300 loss: 0.44101673662662505
  batch 350 loss: 0.4105472081899643
  batch 400 loss: 0.4188436704874039
  batch 450 loss: 0.4201392486691475
  batch 500 loss: 0.5118340188264847
  batch 550 loss: 0.42845427930355073
  batch 600 loss: 0.44992803275585175
  batch 650 loss: 0.44726908057928083
  batch 700 loss: 0.4739856690168381
  batch 750 loss: 0.4502730268239975
  batch 800 loss: 0.4906880694627762
  batch 850 loss: 0.4876953959465027
  batch 900 loss: 0.4392675185203552
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.43927 valid 0.38275, valid PER 10.85%
EPOCH 6:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.4359248697757721
  batch 100 loss: 0.3935300263762474
  batch 150 loss: 0.3629293891787529
  batch 200 loss: 0.3820386049151421
  batch 250 loss: 0.44329466998577116
  batch 300 loss: 0.39859465569257735
  batch 350 loss: 0.37763590037822725
  batch 400 loss: 0.3667588546872139
  batch 450 loss: 0.3565438175201416
  batch 500 loss: 0.38634211391210554
  batch 550 loss: 0.34349593847990034
  batch 600 loss: 0.33214030772447584
  batch 650 loss: 0.3980416426062584
  batch 700 loss: 0.37388150274753573
  batch 750 loss: 0.361289509832859
  batch 800 loss: 0.41990928411483763
  batch 850 loss: 0.3954832801222801
  batch 900 loss: 0.3810697501897812
Adjusting learning rate of group 0 to 8.0000e-05.
LOSS train 0.38107 valid 0.29746, valid PER 9.03%
EPOCH 7:
ETA [8e-05]

isNotFrozen  True
  batch 50 loss: 0.31932512879371644
  batch 100 loss: 0.3202224487066269
  batch 150 loss: 0.2921601903438568
  batch 200 loss: 0.30652218759059907
  batch 250 loss: 0.2854856631159782
  batch 300 loss: 0.28630312651395795
  batch 350 loss: 0.32696204394102096
  batch 400 loss: 0.3226378434896469
  batch 450 loss: 0.3248940739035606
  batch 500 loss: 0.33577960640192034
  batch 550 loss: 0.3125349502265453
  batch 600 loss: 0.34046566754579544
  batch 650 loss: 0.3316351705789566
  batch 700 loss: 0.3492806878685951
  batch 750 loss: 0.3137794531881809
  batch 800 loss: 0.32954199582338334
  batch 850 loss: 0.32166343748569487
  batch 900 loss: 0.3238022711873054
Adjusting learning rate of group 0 to 6.0000e-05.
LOSS train 0.32380 valid 0.28872, valid PER 8.96%
EPOCH 8:
ETA [6.000000000000001e-05]

isNotFrozen  True
  batch 50 loss: 0.2514463749527931
  batch 100 loss: 0.2702785336971283
  batch 150 loss: 0.27499920755624774
  batch 200 loss: 0.26612877428531645
  batch 250 loss: 0.2671832239627838
  batch 300 loss: 0.26757915124297144
  batch 350 loss: 0.28121606886386874
  batch 400 loss: 0.2732919102907181
  batch 450 loss: 0.2565934652090073
  batch 500 loss: 0.2501095440983772
  batch 550 loss: 0.2574744515120983
  batch 600 loss: 0.24782090455293657
  batch 650 loss: 0.24414471670985222
  batch 700 loss: 0.26068463146686555
  batch 750 loss: 0.2554037997126579
  batch 800 loss: 0.2778609046339989
  batch 850 loss: 0.2586414480209351
  batch 900 loss: 0.27180912911891936
Adjusting learning rate of group 0 to 4.0000e-05.
LOSS train 0.27181 valid 0.30291, valid PER 8.88%
EPOCH 9:
ETA [4.000000000000001e-05]

isNotFrozen  True
  batch 50 loss: 0.24955712899565696
  batch 100 loss: 0.2346077200770378
  batch 150 loss: 0.21137564957141877
  batch 200 loss: 0.22274315640330314
  batch 250 loss: 0.2186666040122509
  batch 300 loss: 0.19214450493454932
  batch 350 loss: 0.2188969422876835
  batch 400 loss: 0.22985351882874966
  batch 450 loss: 0.21799421787261963
  batch 500 loss: 0.22729425743222237
  batch 550 loss: 0.21349895656108855
  batch 600 loss: 0.22179969027638435
  batch 650 loss: 0.22835719496011733
  batch 700 loss: 0.23880972877144813
  batch 750 loss: 0.20118487119674683
  batch 800 loss: 0.21889033079147338
  batch 850 loss: 0.2277990075945854
  batch 900 loss: 0.2006356617808342
Adjusting learning rate of group 0 to 2.0000e-05.
LOSS train 0.20064 valid 0.27699, valid PER 8.63%
EPOCH 10:
ETA [2.0000000000000005e-05]

isNotFrozen  True
  batch 50 loss: 0.18353021577000617
  batch 100 loss: 0.19802969560027123
  batch 150 loss: 0.17647728472948074
  batch 200 loss: 0.18886775702238082
  batch 250 loss: 0.18349493503570558
  batch 300 loss: 0.17791489914059638
  batch 350 loss: 0.16732777506113053
  batch 400 loss: 0.17260785214602947
  batch 450 loss: 0.17096540853381156
  batch 500 loss: 0.17681112952530384
  batch 550 loss: 0.18461135059595107
  batch 600 loss: 0.16503795616328717
  batch 650 loss: 0.18819797798991203
  batch 700 loss: 0.16377173468470574
  batch 750 loss: 0.17127068996429443
  batch 800 loss: 0.17385888651013373
  batch 850 loss: 0.1758522507548332
  batch 900 loss: 0.1681628206372261
Adjusting learning rate of group 0 to 0.0000e+00.
LOSS train 0.16816 valid 0.27909, valid PER 7.98%
Training finished in 10.0 minutes.
Model saved to checkpoints/20240131_141534/model_9
Loading model from checkpoints/20240131_141534/model_9
SUB: 5.70%, DEL: 1.95%, INS: 2.52%, COR: 92.35%, PER: 10.16%
