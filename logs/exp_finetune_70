Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10, optimiser='adam', schedule_lr=True, freeze_layers=-1, first_milestone=0.6)
Total number of model parameters is 94402472
Adjusting learning rate of group 0 to 1.0000e-04.
Adjusting learning rate of group 0 to 1.0000e-04.
EPOCH 1:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 10.848146648406983
  batch 100 loss: 8.378839082717896
  batch 150 loss: 6.3344189739227295
  batch 200 loss: 4.834939661026001
  batch 250 loss: 4.123986110687256
  batch 300 loss: 3.7723504114151
  batch 350 loss: 3.596732087135315
  batch 400 loss: 3.512187275886536
  batch 450 loss: 3.4211546897888185
  batch 500 loss: 3.386468472480774
  batch 550 loss: 3.350238184928894
  batch 600 loss: 3.3148741626739504
  batch 650 loss: 3.3140769577026368
  batch 700 loss: 3.3175572633743284
  batch 750 loss: 3.2931177902221678
  batch 800 loss: 3.2801066732406614
  batch 850 loss: 3.290566363334656
  batch 900 loss: 3.25213858127594
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.25214 valid 3.47203, valid PER 99.95%
EPOCH 2:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.2419506978988646
  batch 100 loss: 3.2395229625701902
  batch 150 loss: 3.2251063442230223
  batch 200 loss: 3.214635634422302
  batch 250 loss: 3.2220904064178466
  batch 300 loss: 3.215688796043396
  batch 350 loss: 3.208019328117371
  batch 400 loss: 3.206213216781616
  batch 450 loss: 3.184910230636597
  batch 500 loss: 3.196711764335632
  batch 550 loss: 3.1838263463974
  batch 600 loss: 3.149550061225891
  batch 650 loss: 3.1504725551605226
  batch 700 loss: 3.133625044822693
  batch 750 loss: 3.164659948348999
  batch 800 loss: 3.1203162813186647
  batch 850 loss: 3.1313040924072264
  batch 900 loss: 3.1210364532470702
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 3.12104 valid 3.22656, valid PER 98.00%
EPOCH 3:
ETA [0.0001]

isNotFrozen  False
  batch 50 loss: 3.1024963903427123
  batch 100 loss: 3.0745787620544434
  batch 150 loss: 3.081776080131531
  batch 200 loss: 3.0528219985961913
  batch 250 loss: 3.0183365154266357
  batch 300 loss: 3.0320896816253664
  batch 350 loss: 3.0339503288269043
  batch 400 loss: 3.004957938194275
  batch 450 loss: 3.0153021097183226
  batch 500 loss: 2.9580759239196777
  batch 550 loss: 2.9452329301834106
  batch 600 loss: 2.917998743057251
  batch 650 loss: 2.871694254875183
  batch 700 loss: 2.874258766174316
  batch 750 loss: 2.889163556098938
  batch 800 loss: 2.847153491973877
  batch 850 loss: 2.8662433385849
  batch 900 loss: 2.8085410737991334
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 2.80854 valid 2.82643, valid PER 82.54%
EPOCH 4:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 2.028932774066925
  batch 100 loss: 1.1562358152866363
  batch 150 loss: 0.8161012291908264
  batch 200 loss: 0.7202408212423325
  batch 250 loss: 0.6588013923168182
  batch 300 loss: 0.631198456287384
  batch 350 loss: 0.5645048874616623
  batch 400 loss: 0.6118073731660842
  batch 450 loss: 0.5760813021659851
  batch 500 loss: 0.547020496726036
  batch 550 loss: 0.6146189314126969
  batch 600 loss: 0.5648611360788345
  batch 650 loss: 0.5176089569926262
  batch 700 loss: 0.5076892948150635
  batch 750 loss: 0.5252072620391846
  batch 800 loss: 0.5548886269330978
  batch 850 loss: 0.5224026483297348
  batch 900 loss: 0.5361756151914596
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.53618 valid 0.49326, valid PER 13.33%
EPOCH 5:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.5723818230628968
  batch 100 loss: 0.4986785525083542
  batch 150 loss: 0.491467005610466
  batch 200 loss: 0.454914225935936
  batch 250 loss: 0.4642691248655319
  batch 300 loss: 0.5059524881839752
  batch 350 loss: 0.541729002892971
  batch 400 loss: 0.4655077415704727
  batch 450 loss: 0.4329432338476181
  batch 500 loss: 0.5645179158449173
  batch 550 loss: 0.4845392805337906
  batch 600 loss: 0.43179621756076814
  batch 650 loss: 0.6328266960382462
  batch 700 loss: 0.5167432647943496
  batch 750 loss: 0.45274255275726316
  batch 800 loss: 0.4437924098968506
  batch 850 loss: 0.4379210436344147
  batch 900 loss: 0.4229818904399872
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.42298 valid 0.30441, valid PER 8.97%
EPOCH 6:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.35961377948522566
  batch 100 loss: 0.3605524384975433
  batch 150 loss: 0.3310807977616787
  batch 200 loss: 0.3626872864365578
  batch 250 loss: 0.3766002011299133
  batch 300 loss: 0.384722957611084
  batch 350 loss: 0.38774397909641267
  batch 400 loss: 0.3579274171590805
  batch 450 loss: 0.35234110891819
  batch 500 loss: 0.3494533950090408
  batch 550 loss: 0.33963708221912386
  batch 600 loss: 0.3371863570809364
  batch 650 loss: 0.3671263894438744
  batch 700 loss: 0.3721816551685333
  batch 750 loss: 0.36223693907260895
  batch 800 loss: 0.3874671655893326
  batch 850 loss: 0.3645214906334877
  batch 900 loss: 0.3951629039645195
Adjusting learning rate of group 0 to 1.0000e-04.
LOSS train 0.39516 valid 0.30500, valid PER 9.33%
EPOCH 7:
ETA [0.0001]

isNotFrozen  True
  batch 50 loss: 0.3236896377801895
  batch 100 loss: 0.33534316569566724
  batch 150 loss: 0.3236445151269436
  batch 200 loss: 0.3394920149445534
  batch 250 loss: 0.30529636681079864
  batch 300 loss: 0.34136842340230944
  batch 350 loss: 0.38382133185863493
  batch 400 loss: 0.3578344893455505
  batch 450 loss: 0.4183011171221733
  batch 500 loss: 0.3628600716590881
  batch 550 loss: 0.34319361865520476
  batch 600 loss: 0.31573773592710497
  batch 650 loss: 0.3560880449414253
  batch 700 loss: 0.3590100163221359
  batch 750 loss: 0.32832244247198106
  batch 800 loss: 0.34085252463817595
  batch 850 loss: 0.402442672252655
  batch 900 loss: 0.37209940671920777
Adjusting learning rate of group 0 to 7.5000e-05.
LOSS train 0.37210 valid 0.37804, valid PER 11.20%
EPOCH 8:
ETA [7.500000000000001e-05]

isNotFrozen  True
  batch 50 loss: 0.32772246390581133
  batch 100 loss: 0.3214524501562119
  batch 150 loss: 0.2969727632403374
  batch 200 loss: 0.2740676848590374
  batch 250 loss: 0.30453137755393983
  batch 300 loss: 0.2803313687443733
  batch 350 loss: 0.26904949158430097
  batch 400 loss: 0.28302016347646713
  batch 450 loss: 0.293698470890522
  batch 500 loss: 0.2899862216413021
  batch 550 loss: 0.27132213562726976
  batch 600 loss: 0.28903076678514483
  batch 650 loss: 0.2862133049964905
  batch 700 loss: 0.3128322371840477
  batch 750 loss: 0.26354780480265616
  batch 800 loss: 0.3153483924269676
  batch 850 loss: 0.29548254489898684
  batch 900 loss: 0.3018213015794754
Adjusting learning rate of group 0 to 5.0000e-05.
LOSS train 0.30182 valid 0.30262, valid PER 8.97%
EPOCH 9:
ETA [5.000000000000001e-05]

isNotFrozen  True
  batch 50 loss: 0.24432917252182962
  batch 100 loss: 0.2277255569398403
  batch 150 loss: 0.2363892388343811
  batch 200 loss: 0.23399637520313263
  batch 250 loss: 0.240510895550251
  batch 300 loss: 0.2227305978536606
  batch 350 loss: 0.22653250977396966
  batch 400 loss: 0.35269002050161363
  batch 450 loss: 0.27586390033364294
  batch 500 loss: 0.24717781141400338
  batch 550 loss: 0.24397142648696898
  batch 600 loss: 0.24875564351677895
  batch 650 loss: 0.2479424960911274
  batch 700 loss: 0.24913985446095466
  batch 750 loss: 0.21866967380046845
  batch 800 loss: 0.24586874589323998
  batch 850 loss: 0.23202352747321128
  batch 900 loss: 0.21963815331459047
Adjusting learning rate of group 0 to 2.5000e-05.
LOSS train 0.21964 valid 0.30148, valid PER 9.17%
EPOCH 10:
ETA [2.5000000000000005e-05]

isNotFrozen  True
  batch 50 loss: 0.21724314227700234
  batch 100 loss: 0.20998829260468482
  batch 150 loss: 0.20936355352401734
  batch 200 loss: 0.20678058579564096
  batch 250 loss: 0.20015051856637
  batch 300 loss: 0.1972629664838314
  batch 350 loss: 0.17961307406425475
  batch 400 loss: 0.18925174877047538
  batch 450 loss: 0.19729992061853407
  batch 500 loss: 0.1901319132745266
  batch 550 loss: 0.17526305243372917
  batch 600 loss: 0.20230949983000757
  batch 650 loss: 0.1827914847433567
  batch 700 loss: 0.1860883839428425
  batch 750 loss: 0.18313237518072129
  batch 800 loss: 0.20848154254257678
  batch 850 loss: 0.18936028122901916
  batch 900 loss: 0.16862398833036424
Adjusting learning rate of group 0 to 0.0000e+00.
LOSS train 0.16862 valid 0.28066, valid PER 8.22%
Training finished in 12.0 minutes.
Model saved to checkpoints/20240201_082854/model_10
Loading model from checkpoints/20240201_082854/model_10
SUB: 5.26%, DEL: 2.14%, INS: 1.96%, COR: 92.60%, PER: 9.36%
