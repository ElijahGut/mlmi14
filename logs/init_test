Namespace(seed=123, train_json='data/train.json', val_json='data/dev.json', test_json='data/test.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=20)
Total number of model parameters is 94402472
EPOCH 1:
  batch 50 loss: 5.288840866088867
  batch 100 loss: 3.483549880981445
  batch 150 loss: 3.392970223426819
  batch 200 loss: 3.3526309871673585
  batch 250 loss: 3.339895496368408
  batch 300 loss: 3.313022270202637
  batch 350 loss: 3.30558895111084
  batch 400 loss: 3.31446578502655
  batch 450 loss: 3.3073024559020996
  batch 500 loss: 3.2980159854888917
  batch 550 loss: 3.2938103818893434
  batch 600 loss: 3.282564573287964
  batch 650 loss: 3.28078905582428
  batch 700 loss: 3.3008035469055175
  batch 750 loss: 3.286541266441345
  batch 800 loss: 3.29912721157074
  batch 850 loss: 3.2977506732940673
  batch 900 loss: 3.2849048233032225
LOSS train 3.28490 valid 3.34270, valid PER 100.00%
EPOCH 2:
  batch 50 loss: 3.2827716398239137
  batch 100 loss: 3.282170252799988
  batch 150 loss: 3.282764391899109
  batch 200 loss: 3.2769878101348877
  batch 250 loss: 3.284374923706055
  batch 300 loss: 3.282927436828613
  batch 350 loss: 3.281370425224304
  batch 400 loss: 3.2788703060150146
  batch 450 loss: 3.2849598693847657
  batch 500 loss: 3.2788480806350706
  batch 550 loss: 3.282990617752075
  batch 600 loss: 3.279078707695007
  batch 650 loss: 3.2825017976760864
  batch 700 loss: 3.2661575174331663
  batch 750 loss: 3.285591969490051
  batch 800 loss: 3.2740903186798094
  batch 850 loss: 3.2757694482803346
  batch 900 loss: 3.2822981166839598
LOSS train 3.28230 valid 3.34147, valid PER 100.00%
EPOCH 3:
  batch 50 loss: 3.274067873954773
  batch 100 loss: 3.272553482055664
  batch 150 loss: 3.278672285079956
  batch 200 loss: 3.2749627304077147
  batch 250 loss: 3.263979935646057
  batch 300 loss: 3.283301548957825
  batch 350 loss: 3.2864374256134035
  batch 400 loss: 3.277925424575806
  batch 450 loss: 3.2785474681854248
  batch 500 loss: 3.274278311729431
  batch 550 loss: 3.273411178588867
  batch 600 loss: 3.2613878440856934
  batch 650 loss: 3.2531081438064575
  batch 700 loss: 3.2524299478530883
  batch 750 loss: 3.2838695096969603
  batch 800 loss: 3.2679339981079103
  batch 850 loss: 3.2648428773880003
  batch 900 loss: 3.264097032546997
LOSS train 3.26410 valid 3.32551, valid PER 100.00%
EPOCH 4:
  batch 50 loss: 3.2647079849243164
  batch 100 loss: 3.2608795261383055
  batch 150 loss: 3.2488009929656982
  batch 200 loss: 3.2764749574661254
  batch 250 loss: 3.2733202028274535
  batch 300 loss: 3.26461492061615
  batch 350 loss: 3.2522520208358765
  batch 400 loss: 3.2622455310821534
  batch 450 loss: 3.273561782836914
  batch 500 loss: 3.232513837814331
  batch 550 loss: 3.2676371002197264
  batch 600 loss: 3.2658927202224732
  batch 650 loss: 3.2565062761306764
  batch 700 loss: 3.252401976585388
  batch 750 loss: 3.235629758834839
  batch 800 loss: 3.2365105676651003
  batch 850 loss: 3.2322225332260133
  batch 900 loss: 3.249266004562378
LOSS train 3.24927 valid 3.27684, valid PER 100.00%
EPOCH 5:
  batch 50 loss: 3.229509973526001
  batch 100 loss: 3.229454507827759
  batch 150 loss: 3.218830499649048
  batch 200 loss: 3.225388035774231
  batch 250 loss: 3.2257848167419434
  batch 300 loss: 3.2176890420913695
  batch 350 loss: 3.229470911026001
  batch 400 loss: 3.2260467863082884
  batch 450 loss: 3.207285079956055
  batch 500 loss: 3.217666277885437
  batch 550 loss: 3.199845299720764
  batch 600 loss: 3.1956124830245973
  batch 650 loss: 3.190338931083679
  batch 700 loss: 3.1972387266159057
  batch 750 loss: 3.1841597890853883
  batch 800 loss: 3.1915665102005004
  batch 850 loss: 3.181873803138733
  batch 900 loss: 3.171862564086914
LOSS train 3.17186 valid 3.19289, valid PER 99.76%
EPOCH 6:
  batch 50 loss: 3.143378472328186
  batch 100 loss: 3.155619249343872
  batch 150 loss: 3.130884509086609
  batch 200 loss: 3.124053468704224
  batch 250 loss: 3.1178330993652343
  batch 300 loss: 3.0807019233703614
  batch 350 loss: 3.0929348850250244
  batch 400 loss: 3.06085129737854
  batch 450 loss: 3.054389877319336
  batch 500 loss: 3.0355402994155884
  batch 550 loss: 3.0349377584457398
  batch 600 loss: 3.0145780563354494
  batch 650 loss: 3.011146655082703
  batch 700 loss: 2.9830163097381592
  batch 750 loss: 2.9705670404434206
  batch 800 loss: 2.946866111755371
  batch 850 loss: 2.9367565393447874
  batch 900 loss: 2.920226526260376
LOSS train 2.92023 valid 3.06258, valid PER 85.14%
EPOCH 7:
  batch 50 loss: 2.8842443561553956
  batch 100 loss: 2.8803502416610716
  batch 150 loss: 2.8535393714904784
  batch 200 loss: 2.8344767761230467
  batch 250 loss: 2.8007864952087402
  batch 300 loss: 2.7714062929153442
  batch 350 loss: 2.769283676147461
  batch 400 loss: 2.746937017440796
  batch 450 loss: 2.729708914756775
  batch 500 loss: 2.69322904586792
  batch 550 loss: 2.6776708602905273
  batch 600 loss: 2.672227725982666
  batch 650 loss: 2.616831340789795
  batch 700 loss: 2.595890221595764
  batch 750 loss: 2.575117812156677
  batch 800 loss: 2.5671435356140138
  batch 850 loss: 2.561933145523071
  batch 900 loss: 2.5598637008666993
LOSS train 2.55986 valid 2.79469, valid PER 77.69%
EPOCH 8:
  batch 50 loss: 2.4882627820968626
  batch 100 loss: 2.487146968841553
  batch 150 loss: 2.452780466079712
  batch 200 loss: 2.418947072029114
  batch 250 loss: 2.406606955528259
  batch 300 loss: 2.378188443183899
  batch 350 loss: 2.3663207578659056
  batch 400 loss: 2.3322803020477294
  batch 450 loss: 2.334879879951477
  batch 500 loss: 2.3048501873016356
  batch 550 loss: 2.262467427253723
  batch 600 loss: 2.2687025260925293
  batch 650 loss: 2.248621368408203
  batch 700 loss: 2.216831169128418
  batch 750 loss: 2.1975968980789187
  batch 800 loss: 2.1753923535346984
  batch 850 loss: 2.188217833042145
  batch 900 loss: 2.1415303444862364
LOSS train 2.14153 valid 2.53125, valid PER 61.57%
EPOCH 9:
  batch 50 loss: 2.087969546318054
  batch 100 loss: 2.088102662563324
  batch 150 loss: 2.0657794070243836
  batch 200 loss: 2.0459324669837953
  batch 250 loss: 2.014178786277771
  batch 300 loss: 2.0350463128089906
  batch 350 loss: 1.9998761367797853
  batch 400 loss: 1.9814189195632934
  batch 450 loss: 1.9665543365478515
  batch 500 loss: 1.9241391324996948
  batch 550 loss: 1.912707793712616
  batch 600 loss: 1.885899178981781
  batch 650 loss: 1.8755033135414123
  batch 700 loss: 1.8958168840408325
  batch 750 loss: 1.847798340320587
  batch 800 loss: 1.8371323609352113
  batch 850 loss: 1.821200647354126
  batch 900 loss: 1.807437756061554
LOSS train 1.80744 valid 2.33468, valid PER 50.85%
EPOCH 10:
  batch 50 loss: 1.7786409997940062
  batch 100 loss: 1.7421719789505006
  batch 150 loss: 1.7371908473968505
  batch 200 loss: 1.7478035974502564
  batch 250 loss: 1.7253379392623902
  batch 300 loss: 1.7123629140853882
  batch 350 loss: 1.680954167842865
  batch 400 loss: 1.651558518409729
  batch 450 loss: 1.6024981999397279
  batch 500 loss: 1.635138919353485
  batch 550 loss: 1.6342582154273986
  batch 600 loss: 1.5979307413101196
  batch 650 loss: 1.588902792930603
  batch 700 loss: 1.5525530648231507
  batch 750 loss: 1.5518180322647095
  batch 800 loss: 1.5591144824028016
  batch 850 loss: 1.5089473223686218
  batch 900 loss: 1.5203431463241577
LOSS train 1.52034 valid 1.97241, valid PER 39.11%
EPOCH 11:
  batch 50 loss: 1.4968763589859009
  batch 100 loss: 1.4819439029693604
  batch 150 loss: 1.4490111136436463
  batch 200 loss: 1.4410248494148254
  batch 250 loss: 1.4591429924964905
  batch 300 loss: 1.417844021320343
  batch 350 loss: 1.3904265570640564
  batch 400 loss: 1.4079742193222047
  batch 450 loss: 1.3904291009902954
  batch 500 loss: 1.346711778640747
  batch 550 loss: 1.3599685001373292
  batch 600 loss: 1.357933931350708
  batch 650 loss: 1.3400984930992126
  batch 700 loss: 1.3133716082572937
  batch 750 loss: 1.305530582666397
  batch 800 loss: 1.3213616681098939
  batch 850 loss: 1.3190340065956117
  batch 900 loss: 1.3049403977394105
LOSS train 1.30494 valid 1.76135, valid PER 33.92%
EPOCH 12:
  batch 50 loss: 1.2640391564369202
  batch 100 loss: 1.2699952602386475
  batch 150 loss: 1.2368277978897095
  batch 200 loss: 1.2183271336555481
  batch 250 loss: 1.2388729810714723
  batch 300 loss: 1.2142223346233367
  batch 350 loss: 1.2094829046726228
  batch 400 loss: 1.2169412851333619
  batch 450 loss: 1.1765419435501099
  batch 500 loss: 1.1885831475257873
  batch 550 loss: 1.1528014969825744
  batch 600 loss: 1.160904210805893
  batch 650 loss: 1.1705533397197723
  batch 700 loss: 1.143951895236969
  batch 750 loss: 1.1441358017921448
  batch 800 loss: 1.1094832587242127
  batch 850 loss: 1.1285626804828643
  batch 900 loss: 1.1106119537353516
LOSS train 1.11061 valid 1.45976, valid PER 28.13%
EPOCH 13:
  batch 50 loss: 1.0734024262428283
  batch 100 loss: 1.1095492386817931
  batch 150 loss: 1.0489300680160523
  batch 200 loss: 1.0783174288272859
  batch 250 loss: 1.0576519763469696
  batch 300 loss: 1.0551399111747741
  batch 350 loss: 1.046953946352005
  batch 400 loss: 1.0372359549999237
  batch 450 loss: 1.0530329298973085
  batch 500 loss: 1.0161354184150695
  batch 550 loss: 1.0136083948612213
  batch 600 loss: 1.021220841407776
  batch 650 loss: 0.9933819854259491
  batch 700 loss: 0.9787032389640808
  batch 750 loss: 0.9792098021507263
  batch 800 loss: 0.9508346450328827
  batch 850 loss: 0.972603120803833
  batch 900 loss: 0.973694543838501
LOSS train 0.97369 valid 1.24278, valid PER 21.96%
EPOCH 14:
  batch 50 loss: 0.9346864366531372
  batch 100 loss: 0.9522834658622742
  batch 150 loss: 0.9405516660213471
  batch 200 loss: 0.9302217495441437
  batch 250 loss: 0.9182670795917511
  batch 300 loss: 0.9327287364006043
  batch 350 loss: 0.9322125089168548
  batch 400 loss: 0.9213263964653016
  batch 450 loss: 0.9123961293697357
  batch 500 loss: 0.9105888760089874
  batch 550 loss: 0.8988692891597748
  batch 600 loss: 0.8953489208221436
  batch 650 loss: 0.9007289946079254
  batch 700 loss: 0.8956137990951538
  batch 750 loss: 0.8757476544380188
  batch 800 loss: 0.8598606753349304
  batch 850 loss: 0.8748659074306488
  batch 900 loss: 0.8746527051925659
LOSS train 0.87465 valid 1.08973, valid PER 18.40%
EPOCH 15:
  batch 50 loss: 0.8651959049701691
  batch 100 loss: 0.8757495820522309
  batch 150 loss: 0.8376813066005707
  batch 200 loss: 0.8821720218658448
  batch 250 loss: 0.836192797422409
  batch 300 loss: 0.8220615720748902
  batch 350 loss: 0.8357283079624176
  batch 400 loss: 0.8157984638214111
  batch 450 loss: 0.7916500496864319
  batch 500 loss: 0.8058019304275512
  batch 550 loss: 0.8185832273960113
  batch 600 loss: 0.8375824022293091
  batch 650 loss: 0.8308697926998139
  batch 700 loss: 0.8041030871868133
  batch 750 loss: 0.8063208627700805
  batch 800 loss: 0.779361457824707
  batch 850 loss: 0.7851915609836578
  batch 900 loss: 0.7897052299976349
LOSS train 0.78971 valid 1.05494, valid PER 16.03%
EPOCH 16:
  batch 50 loss: 0.7776232767105102
  batch 100 loss: 0.778271632194519
  batch 150 loss: 0.7892293441295624
  batch 200 loss: 0.752516131401062
  batch 250 loss: 0.7624335098266601
  batch 300 loss: 0.7783770668506622
  batch 350 loss: 0.8026786959171295
  batch 400 loss: 0.7791719603538513
  batch 450 loss: 0.7644590127468109
  batch 500 loss: 0.7394429647922516
  batch 550 loss: 0.7767421460151672
  batch 600 loss: 0.7353740549087524
  batch 650 loss: 0.7164337587356567
  batch 700 loss: 0.7407641988992691
  batch 750 loss: 0.7434273672103882
  batch 800 loss: 0.7512695801258087
  batch 850 loss: 0.7414441752433777
  batch 900 loss: 0.7231960296630859
LOSS train 0.72320 valid 0.79689, valid PER 14.62%
EPOCH 17:
  batch 50 loss: 0.7290476536750794
  batch 100 loss: 0.7165209650993347
  batch 150 loss: 0.7170069193840027
  batch 200 loss: 0.6804011183977127
  batch 250 loss: 0.7367246258258819
  batch 300 loss: 0.7157782125473022
  batch 350 loss: 0.6826435548067092
  batch 400 loss: 0.7306727814674377
  batch 450 loss: 0.7218197602033615
  batch 500 loss: 0.6921619784832
  batch 550 loss: 0.7132571685314179
  batch 600 loss: 0.7216018217802048
  batch 650 loss: 0.7155415618419647
  batch 700 loss: 0.7020925098657608
  batch 750 loss: 0.6775494813919067
  batch 800 loss: 0.681683161854744
  batch 850 loss: 0.6574911004304886
  batch 900 loss: 0.6922945767641068
LOSS train 0.69229 valid 0.80906, valid PER 14.25%
EPOCH 18:
  batch 50 loss: 0.6812894189357758
  batch 100 loss: 0.6556043440103531
  batch 150 loss: 0.6947246760129928
  batch 200 loss: 0.6628095149993897
  batch 250 loss: 0.6553521913290024
  batch 300 loss: 0.6687926262617111
  batch 350 loss: 0.6615321999788284
  batch 400 loss: 0.66572922706604
  batch 450 loss: 0.6921747255325318
  batch 500 loss: 0.6579621821641922
  batch 550 loss: 0.6741086518764496
  batch 600 loss: 0.6463655704259872
  batch 650 loss: 0.6489245784282685
  batch 700 loss: 0.6453579163551331
  batch 750 loss: 0.6579998922348023
  batch 800 loss: 0.6239543068408966
  batch 850 loss: 0.631898759007454
  batch 900 loss: 0.6489995896816254
LOSS train 0.64900 valid 0.65533, valid PER 13.10%
EPOCH 19:
  batch 50 loss: 0.6135606372356415
  batch 100 loss: 0.6242440986633301
  batch 150 loss: 0.6381692129373551
  batch 200 loss: 0.6222415632009506
  batch 250 loss: 0.6343994629383087
  batch 300 loss: 0.6097431534528732
  batch 350 loss: 0.5996493399143219
  batch 400 loss: 0.6327780771255493
  batch 450 loss: 0.6167938274145126
  batch 500 loss: 0.6168430835008621
  batch 550 loss: 0.6175194144248962
  batch 600 loss: 0.6252763104438782
  batch 650 loss: 0.6250012254714966
  batch 700 loss: 0.605172181725502
  batch 750 loss: 0.5889073282480239
  batch 800 loss: 0.6390487796068192
  batch 850 loss: 0.5972340953350067
  batch 900 loss: 0.6170850056409836
LOSS train 0.61709 valid 0.57755, valid PER 12.60%
EPOCH 20:
  batch 50 loss: 0.5794260758161545
  batch 100 loss: 0.6085559523105621
  batch 150 loss: 0.5878220707178116
  batch 200 loss: 0.5893880110979081
  batch 250 loss: 0.5906506824493408
  batch 300 loss: 0.5915828943252563
  batch 350 loss: 0.5586498391628265
  batch 400 loss: 0.6279952692985534
  batch 450 loss: 0.5801448780298233
  batch 500 loss: 0.5556305575370789
  batch 550 loss: 0.6072916269302369
  batch 600 loss: 0.5634443026781082
  batch 650 loss: 0.5714721101522445
  batch 700 loss: 0.5748960709571839
  batch 750 loss: 0.580905909538269
  batch 800 loss: 0.5759152942895889
  batch 850 loss: 0.6010221993923187
  batch 900 loss: 0.5607013785839081
LOSS train 0.56070 valid 0.52023, valid PER 11.85%
Training finished in 24.0 minutes.
Model saved to checkpoints/20240127_220426/model_20
Loading model from checkpoints/20240127_220426/model_20
SUB: 5.87%, DEL: 3.41%, INS: 3.98%, COR: 90.73%, PER: 13.25%
